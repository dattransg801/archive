## XGBoost  {.unnumbered}

<h3 class="course__description-title">Sergey Fogelson</h3>
<p class="course__instructor-description display-none-mobile-course-page-experiment">
    Sergey began his career as an academic at Dartmouth College, where he researched the neural bases of visual category learning and obtained his Ph.D. in Cognitive Neuroscience.
After leaving academia, Sergey got into the rapidly growing startup scene in the NYC metro area, where he has worked as a data scientist in digital advertising, cybersecurity, finance, and media. He is heavily involved in the NYC-area teaching community and has taught courses at various bootcamps, and has been a volunteer teacher in computer science through TEALSK12. When Sergey is not working or teaching, he is probably hiking. (He thru-hiked the Appalachian trail before graduate school).
  </p>

**Course Description**

<p class="course__description">Do you know the basics of supervised learning and want to use state-of-the-art models on real-world datasets? Gradient boosting is currently one of the most popular techniques for efficient modeling of tabular datasets of all sizes. XGboost is a very fast, scalable implementation of gradient boosting, with models using XGBoost regularly winning online data science competitions and being used at scale across different industries. In this course, you'll learn how to use this powerful library alongside pandas and scikit-learn to build and tune supervised learning models. You'll work with real-world datasets to solve classification and regression problems.</p>

### Classification {.unnumbered}

<p class="chapter__description">
    This chapter will introduce you to the fundamental idea behind XGBoostâ€”boosted learners. Once you understand how XGBoost works, you'll apply it to solve a common classification problem found in industry: predicting whether a customer will stop being a customer at some point in the future.
  </p>

#### Welcome to the course! {.unnumbered}



##### Which of these is a classification problem? {.unnumbered}

<p>Given below are 4 potential machine learning problems you might encounter in the wild. Pick the one that is a classification problem.</p>

<li>Given past performance of stocks and various other financial data, predicting the exact price of a given stock (Google) tomorrow.</li>
<li>Given a large dataset of user behaviors on a website, generating an informative segmentation of the users based on their behaviors.</li>
<strong><li>Predicting whether a given user will click on an ad given the ad content and metadata associated with the user.</li></strong>
<li>Given a user's past behavior on a video platform, presenting him/her with a series of recommended videos to watch next.</li>

<p class="dc-completion-pane__message dc-u-maxw-100pc">Well done! This is indeed a classification problem.</p>

##### Which of these is a binary classification problem? {.unnumbered}

<p>Great! A classification problem involves predicting the category a given data point belongs to out of a finite set of possible categories. Depending on how many possible categories there are to predict, a classification problem can be either binary or multi-class. Let's do another quick refresher here. Your job is to pick the <strong>binary</strong> classification problem out of the following list of supervised learning problems.</p>

<strong><li>Predicting whether a given image contains a cat.</li></strong>
<li>Predicting the emotional valence of a sentence (Valence can be positive, negative, or neutral).</li>
<li>Recommending the most tax-efficient strategy for tax filing in an automated accounting system.</li>
<li>Given a list of symptoms, generating a rank-ordered list of most likely diseases.</li>

<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct! A binary classification problem involves picking between 2 choices.</p>

#### Introducing XGBoost {.unnumbered}



##### XGBoost: Fit/Predict {.unnumbered}


<div class>
<p>It's time to create your first XGBoost model! As Sergey showed you in the video, you can use the scikit-learn <code>.fit()</code> / <code>.predict()</code> paradigm that you are already familiar to build your XGBoost models, as the <code>xgboost</code> library has a scikit-learn compatible API!</p>
<p>Here, you'll be working with churn data. This dataset contains imaginary data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up. It has been pre-loaded for you into a DataFrame called <code>churn_data</code> - explore it in the Shell!</p>
<p>Your goal is to use the first month's worth of data to predict whether the app's users will remain users of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, you'll split the data into training and test sets, fit a small <code>xgboost</code> model on the training set, and evaluate its performance on the test set by computing its accuracy.</p>
<p><code>pandas</code> and <code>numpy</code> have been imported as <code>pd</code> and <code>np</code>, and <code>train_test_split</code> has been imported from <code>sklearn.model_selection</code>. Additionally, the arrays for the features and the target have been created as <code>X</code> and <code>y</code>.</p>
</div>

<li>Import <code>xgboost</code> as <code>xgb</code>.</li>


<li>Create training and test sets such that 20% of the data is used for testing. Use a <code>random_state</code> of <code>123</code>.</li>


<li>Instantiate an <code>XGBoostClassifier</code> as <code>xg_cl</code> using <code>xgb.XGBClassifier()</code>. Specify <code>n_estimators</code> to be <code>10</code> estimators and an <code>objective</code> of <code>'binary:logistic'</code>. Do not worry about what this means just yet, you will learn about these parameters later in this course.</li>


<li>Fit <code>xg_cl</code> to the training set (<code>X_train, y_train)</code> using the <code>.fit()</code> method.</li>


<li>Predict the labels of the test set (<code>X_test</code>) using the <code>.predict()</code> method and hit 'Submit Answer' to print the accuracy.</li>
```{python}
# edited/added
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
churn_data = pd.read_csv("archive/Extreme-Gradient-Boosting-with-XGBoost/datasets/churn_data.csv")

# import xgboost
import xgboost as xgb

# Create arrays for the features and the target: X, y
X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]

# Create the training and test sets
X_train,X_test,y_train,y_test= train_test_split(X, y, test_size=0.2, random_state=123)

# Instantiate the XGBClassifier: xg_cl
xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)

# Fit the classifier to the training set
xg_cl.fit(X_train,y_train)

# Predict the labels of the test set: preds
preds = xg_cl.predict(X_test)

# Compute the accuracy: accuracy
accuracy = float(np.sum(preds==y_test))/y_test.shape[0]
print("accuracy: %f" % (accuracy))
```

<p class="">Well done! Your model has an accuracy of around 74%. In Chapter 3, you'll learn about ways to fine tune your XGBoost models. For now, let's refresh our memories on how decision trees work. See you in the next video!</p>


#### Decision tree {.unnumbered}



##### Decision trees {.unnumbered}


<div class>
<p>Your task in this exercise is to make a simple decision tree using scikit-learn's <code>DecisionTreeClassifier</code> on the <code>breast cancer</code> dataset that comes pre-loaded with scikit-learn. </p>
<p>This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign). </p>
<p>We've preloaded the dataset of samples (measurements) into <code>X</code> and the target values per tumor into <code>y</code>. Now, you have to split the complete dataset into training and testing sets, and then train a <code>DecisionTreeClassifier</code>. You'll specify a parameter called <code>max_depth</code>. Many other parameters can be modified within this model, and you can check all of them out <a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier">here</a>.</p>
</div>


<li>Import:<ul>
<li>
<code>train_test_split</code> from <code>sklearn.model_selection</code>.</li>


<li>
<code>DecisionTreeClassifier</code> from <code>sklearn.tree</code>.</li>


</ul>
</li>


<li>Create training and test sets such that 20% of the data is used for testing. Use a <code>random_state</code> of <code>123</code>.</li>


<li>Instantiate a <code>DecisionTreeClassifier</code> called <code>dt_clf_4</code> with a <code>max_depth</code> of <code>4</code>. This parameter specifies the maximum number of successive split points you can have before reaching a leaf node.</li>


<li>Fit the classifier to the training set and predict the labels of the test set.</li>
```{python}
# edited/added
breast_cancer = pd.read_csv("archive/Extreme-Gradient-Boosting-with-XGBoost/datasets/breast_cancer.csv")
X = breast_cancer.iloc[:,2:].to_numpy()
y = np.array([0 if i == "M" else 1 for i in breast_cancer.iloc[:,1]])

# Import the necessary modules
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# Create the training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# Instantiate the classifier: dt_clf_4
dt_clf_4 = DecisionTreeClassifier(max_depth=4)

# Fit the classifier to the training set
dt_clf_4.fit(X_train, y_train)

# Predict the labels of the test set: y_pred_4
y_pred_4 = dt_clf_4.predict(X_test)

# Compute the accuracy of the predictions: accuracy
accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]
print("accuracy:", accuracy)
```

<p class="">Great work! It's now time to learn about what gives XGBoost its state-of-the-art performance: Boosting.</p>


#### Boosting {.unnumbered}



##### Measuring accuracy {.unnumbered}


<div class>
<p>You'll now practice using XGBoost's learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a <code>DMatrix</code>.</p>
<p>In the previous exercise, the input datasets were converted into <code>DMatrix</code> data on the fly, but when you use the <code>xgboost</code> <code>cv</code> object, you have to first explicitly convert your data into a <code>DMatrix</code>. So, that's what you will do here before running cross-validation on <code>churn_data</code>.</p>
</div>

<li>Create a <code>DMatrix</code> called <code>churn_dmatrix</code> from <code>churn_data</code> using <code>xgb.DMatrix()</code>. The features are available in <code>X</code> and the labels in <code>y</code>.</li>


<li>Perform 3-fold cross-validation by calling <code>xgb.cv()</code>. <code>dtrain</code> is your <code>churn_dmatrix</code>, <code>params</code> is your parameter dictionary, <code>nfold</code> is the number of cross-validation folds (<code>3</code>), <code>num_boost_round</code> is the number of trees we want to build (<code>5</code>), <code>metrics</code> is the metric you want to compute (this will be <code>"error"</code>, which we will convert to an accuracy).</li>
```{python}
# Create arrays for the features and the target: X, y
X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]

# Create the DMatrix from X and y: churn_dmatrix
churn_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:logistic", "max_depth":3}

# Perform cross-validation: cv_results
cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, 
                    nfold=3, num_boost_round=5, 
                    metrics="error", as_pandas=True, seed=123)

# Print cv_results
print(cv_results)

# Print the accuracy
print(((1-cv_results["test-error-mean"]).iloc[-1]))
```

<p class="">Nice work. <code>cv_results</code> stores the training and test mean and standard deviation of the error per boosting round (tree built) as a DataFrame. From <code>cv_results</code>, the final round <code>'test-error-mean'</code> is extracted and converted into an accuracy, where accuracy is <code>1-error</code>. The final accuracy of around 75% is an improvement from earlier!</p>


##### Measuring AUC {.unnumbered}


<div class>
<p>Now that you've used cross-validation to compute average out-of-sample accuracy (after converting from an error), it's very easy to compute any other metric you might be interested in. All you have to do is pass it (or a list of metrics) in as an argument to the <code>metrics</code> parameter of <code>xgb.cv()</code>. </p>
<p>Your job in this exercise is to compute another common metric used in binary classification - the area under the curve (<code>"auc"</code>). As before, <code>churn_data</code> is available in your workspace, along with the DMatrix <code>churn_dmatrix</code> and parameter dictionary <code>params</code>.</p>
</div>

<li>Perform 3-fold cross-validation with <code>5</code> boosting rounds and <code>"auc"</code> as your metric.</li>


<li>Print the <code>"test-auc-mean"</code> column of <code>cv_results</code>.</li>
```{python}
# Perform cross_validation: cv_results
cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, 
                    nfold=3, num_boost_round=5, 
                    metrics="auc", as_pandas=True, seed=123)

# Print cv_results
print(cv_results)

# Print the AUC
print((cv_results["test-auc-mean"]).iloc[-1])
```

<p class="">Fantastic! An AUC of 0.84 is quite strong. As you have seen, XGBoost's learning API makes it very easy to compute any metric you may be interested in. In Chapter 3, you'll learn about techniques to fine-tune your XGBoost models to improve their performance even further. For now, it's time to learn a little about exactly <strong>when</strong> to use XGBoost.</p>


#### XGBoost? {.unnumbered}



##### Using XGBoost {.unnumbered}

<p>XGBoost is a powerful library that scales very well to many samples and works for a variety of supervised learning problems. That said, as Sergey described in the video, you shouldn't always pick it as your default machine learning library when starting a new project, since there are some situations in which it is not the best option. In this exercise, your job is to consider the below examples and select the one which would be the best use of XGBoost.</p>

<li>Visualizing the similarity between stocks by comparing the time series of their historical prices relative to each other.</li>
<li>Predicting whether a person will develop cancer using genetic data with millions of genes, 23 examples of genomes of people that didn't develop cancer, 3 genomes of people who wound up getting cancer.</li>
<li>Clustering documents into topics based on the terms used in them.</li>
<strong><li>Predicting the likelihood that a given user will click an ad from a very large clickstream log with millions of users and their web interactions.</li></strong>

<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct! Way to end the chapter. Time to apply XGBoost to solve regression problems!</p>

### Regression {.unnumbered}

<p class="chapter__description">
    After a brief review of supervised regression, you'll apply XGBoost to the regression task of predicting house prices in Ames, Iowa. You'll learn about the two kinds of base learners that XGboost can use as its weak learners, and review how to evaluate the quality of your regression models.
  </p>

#### Review {.unnumbered}



##### Which of these is a regression problem? {.unnumbered}

<p>Here are 4 potential machine learning problems you might encounter in the wild. Pick the one that is a clear example of a regression problem.</p>

<li>Recommending a restaurant to a user given their past history of restaurant visits and reviews for a dining aggregator app.</li>
<li>Predicting which of several thousand diseases a given person is most likely to have given their symptoms.</li>
<li>Tagging an email as spam/not spam based on its content and metadata (sender, time sent, etc.).</li>
<strong><li>Predicting the expected payout of an auto insurance claim given claim properties (car, accident type, driver prior history, etc.).</li></strong>

<p class="dc-completion-pane__message dc-u-maxw-100pc">Well done! This is indeed an example of a regression problem.</p>

#### Objective (loss) functions and base learners {.unnumbered}



##### Decision trees as base learners {.unnumbered}


<div class>
<p>It's now time to build an XGBoost model to predict house prices - not in Boston, Massachusetts, as you saw in the video, but in Ames, Iowa! This dataset of housing prices has been pre-loaded into a DataFrame called <code>df</code>. If you explore it in the Shell, you'll see that there are a variety of features about the house and its location in the city.</p>
<p>In this exercise, your goal is to use trees as base learners. By default, XGBoost uses trees as base learners, so you don't have to specify that you want to use trees here with <code>booster="gbtree"</code>.</p>
<p><code>xgboost</code> has been imported as <code>xgb</code> and the arrays for the features and the target are available in <code>X</code> and <code>y</code>, respectively.</p>
</div>


<li>Split <code>df</code> into training and testing sets, holding out 20% for testing. Use a <code>random_state</code> of <code>123</code>.</li>


<li>Instantiate the <code>XGBRegressor</code> as <code>xg_reg</code>, using a <code>seed</code> of <code>123</code>. Specify an objective of <code>"reg:linear"</code> and use 10 trees. Note: You don't have to specify <code>booster="gbtree"</code> as this is the default.</li>


<li>Fit <code>xg_reg</code> to the training data and predict the labels of the test set. Save the predictions in a variable called <code>preds</code>.</li>


<li>Compute the <code>rmse</code> using <code>np.sqrt()</code> and the <code>mean_squared_error()</code> function from <code>sklearn.metrics</code>, which has been pre-imported.</li>
```{python}
# edited/added
from sklearn.metrics import mean_squared_error
df = pd.read_csv("archive/Extreme-Gradient-Boosting-with-XGBoost/datasets/ames_housing_trimmed_processed.csv")
X = df.iloc[:,:-1]
y = df.iloc[:,-1]

# Create the training and test sets
X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)

# Instantiate the XGBRegressor: xg_reg
xg_reg = xgb.XGBRegressor(objective="reg:linear", n_estimators=10, seed=123)

# Fit the regressor to the training set
xg_reg.fit(X_train, y_train)

# Predict the labels of the test set: preds
preds = xg_reg.predict(X_test)

# Compute the rmse: rmse
rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" % (rmse))
```

<p class="">Well done! Next, you'll train an XGBoost model using linear base learners and XGBoost's learning API. Will it perform better or worse?</p>


##### Linear base learners {.unnumbered}


<div class>
<p>Now that you've used trees as base models in XGBoost, let's use the other kind of base model that can be used with XGBoost - a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost's powerful learning API. However, because it's uncommon, you have to use XGBoost's own non-scikit-learn compatible functions to build the model, such as <code>xgb.train()</code>. </p>
<p>In order to do this you must create the parameter dictionary that describes the kind of booster you want to use (similarly to how <a href="https://campus.datacamp.com/courses/extreme-gradient-boosting-with-xgboost/10555?ex=9">you created the dictionary in Chapter 1</a> when you used <code>xgb.cv()</code>). The key-value pair that defines the booster type (base model) you need is <code>"booster":"gblinear"</code>.</p>
<p>Once you've created the model, you can use the <code>.train()</code> and <code>.predict()</code> methods of the model just like you've done in the past.</p>
<p>Here, the data has already been split into training and testing sets, so you can dive right into creating the <code>DMatrix</code> objects required by the XGBoost learning API.</p>
</div>

<li>Create two <code>DMatrix</code> objects - <code>DM_train</code> for the training set (<code>X_train</code> and <code>y_train</code>), and <code>DM_test</code> (<code>X_test</code> and <code>y_test</code>) for the test set.</li>


<li>Create a parameter dictionary that defines the <code>"booster"</code> type you will use (<code>"gblinear"</code>) as well as the <code>"objective"</code> you will minimize (<code>"reg:linear"</code>).</li>


<li>Train the model using <code>xgb.train()</code>. You have to specify arguments for the following parameters: <code>params</code>, <code>dtrain</code>, and <code>num_boost_round</code>. Use <code>5</code> boosting rounds. </li>


<li>Predict the labels on the test set using <code>xg_reg.predict()</code>, passing it <code>DM_test</code>. Assign to <code>preds</code>.</li>


<li>Hit 'Submit Answer' to view the RMSE!</li>
```{python}
# Convert the training and testing sets into DMatrixes: DM_train, DM_test
DM_train = xgb.DMatrix(data=X_train, label=y_train)
DM_test =  xgb.DMatrix(data=X_test, label=y_test)

# Create the parameter dictionary: params
params = {"booster":"gblinear", "objective":"reg:linear"}

# Train the model: xg_reg
xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)

# Predict the labels of the test set: preds
preds = xg_reg.predict(DM_test)

# Compute and print the RMSE
rmse = np.sqrt(mean_squared_error(y_test,preds))
print("RMSE: %f" % (rmse))
```

<p class="">Interesting - it looks like linear base learners performed better!</p>


##### Evaluating model quality {.unnumbered}


<div class>
<p>It's now time to begin evaluating model quality.  </p>
<p>Here, you will compare the RMSE and MAE of a cross-validated XGBoost model on the Ames housing data. As in previous exercises, all necessary modules have been pre-loaded and the data is available in the DataFrame <code>df</code>.</p>
</div>

<li>Perform 4-fold cross-validation with <code>5</code> boosting rounds and <code>"rmse"</code> as the metric.</li>


<li>Extract and print the final boosting round RMSE.</li>
```{python}
# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear", "max_depth":4}

# Perform cross-validation: cv_results
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics="rmse", as_pandas=True, seed=123)

# Print cv_results
print(cv_results)

# Extract and print final round boosting round metric
print((cv_results["test-rmse-mean"]).tail(1))
```



<div class="exercise--instructions__content"><p>Now, adapt your code to compute the <code>"mae"</code> instead of the <code>"rmse"</code>.</p></div>
```{python}
# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear", "max_depth":4}

# Perform cross-validation: cv_results
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics="mae", as_pandas=True, seed=123)

# Print cv_results
print(cv_results)

# Extract and print final round boosting round metric
print((cv_results["test-mae-mean"]).tail(1))
```

<p class="">Great work!</p>

#### Regularization {.unnumbered}



##### Using regularization in XGBoost {.unnumbered}


<div class><p>Having seen an example of l1 regularization in the video, you'll now vary the l2 regularization penalty - also known as <code>"lambda"</code> - and see its effect on overall model performance on the Ames housing dataset.</p></div>

<li>Create your <code>DMatrix</code> from <code>X</code> and <code>y</code> as before.</li>


<li>Create an initial parameter dictionary specifying an <code>"objective"</code> of <code>"reg:linear"</code> and <code>"max_depth"</code> of <code>3</code>.</li>


<li>Use <code>xgb.cv()</code> inside of a <code>for</code> loop and systematically vary the <code>"lambda"</code> value by passing in the current l2 value (<code>reg</code>).</li>


<li>Append the <code>"test-rmse-mean"</code> from the last boosting round for each cross-validated <code>xgboost</code> model.</li>


<li>Hit 'Submit Answer' to view the results. What do you notice?</li>
```{python}
# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

reg_params = [1, 10, 100]

# Create the initial parameter dictionary for varying l2 strength: params
params = {"objective":"reg:linear","max_depth":3}

# Create an empty list for storing rmses as a function of l2 complexity
rmses_l2 = []

# Iterate over reg_params
for reg in reg_params:

    # Update l2 strength
    params["lambda"] = reg
    
    # Pass this updated param dictionary into cv
    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics="rmse", as_pandas=True, seed=123)
    
    # Append best rmse (final round) to rmses_l2
    rmses_l2.append(cv_results_rmse["test-rmse-mean"].tail(1).values[0])
    
# Look at best rmse per l2 param
print("Best rmse as a function of l2:")
print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=["l2","rmse"]))
```

<p class="">Nice work! It looks like as as the value of <code>'lambda'</code> increases, so does the RMSE.</p>


##### Visualizing individual XGBoost trees {.unnumbered}


<div class>
<p>Now that you've used XGBoost to both build and evaluate regression as well as classification models, you should get a handle on how to visually explore your models. Here, you will visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset.</p>
<p>XGBoost has a <code>plot_tree()</code> function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the <code>plot_tree()</code> function along with the number of trees you want to plot using the <code>num_trees</code> argument.</p>
</div>


<li>Create a parameter dictionary with an <code>"objective"</code> of <code>"reg:linear"</code> and a <code>"max_depth"</code> of <code>2</code>.</li>


<li>Train the model using <code>10</code> boosting rounds and the parameter dictionary you created. Save the result in <code>xg_reg</code>.</li>


<li>Plot the first tree using <code>xgb.plot_tree()</code>. It takes in two arguments - the model (in this case, <code>xg_reg</code>), and <code>num_trees</code>, which is 0-indexed. So to plot the first tree, specify <code>num_trees=0</code>.</li>


<li>Plot the fifth tree.</li>


<li>Plot the last (tenth) tree sideways. To do this, specify the additional keyword argument <code>rankdir="LR"</code>.</li>
```{python}
# edited/added
import graphviz
import matplotlib.pyplot as plt

# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear", "max_depth":2}

# Train the model: xg_reg
xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)

# Plot the first tree
xgb.plot_tree(xg_reg, num_trees=0)
plt.show()

# Plot the fifth tree
xgb.plot_tree(xg_reg, num_trees=4)
plt.show()

# Plot the last tree sideways
xgb.plot_tree(xg_reg, num_trees=9, rankdir='LR')
plt.show()
```

<p class="">Excellent! Have a look at each of the plots. They provide insight into how the model arrived at its final decisions and what splits it made to arrive at those decisions. This allows us to identify which features are the most important in determining house price. In the next exercise, you'll learn another way of visualizing feature importances.</p>


##### Visualizing feature importances: What features are most important in my dataset {.unnumbered}


<div class>
<p>Another way to visualize your XGBoost models is to examine the importance of each feature column in the original dataset within the model. </p>
<p>One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a <code>plot_importance()</code> function that allows you to do exactly this, and you'll get a chance to use it in this exercise!</p>
</div>


<li>Create your <code>DMatrix</code> from <code>X</code> and <code>y</code> as before.</li>


<li>Create a parameter dictionary with appropriate <code>"objective"</code> (<code>"reg:linear"</code>) and a <code>"max_depth"</code> of <code>4</code>.</li>


<li>Train the model with <code>10</code> boosting rounds, exactly as you did in the previous exercise.</li>


<li>Use <code>xgb.plot_importance()</code> and pass in the trained model to generate the graph of feature importances.</li>
```{python}
# edited/added
import matplotlib.pyplot as plt

# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear", "max_depth":4}

# Train the model: xg_reg
xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)

# Plot the feature importances
xgb.plot_importance(xg_reg)
plt.show()
```

<p class="">Brilliant! It looks like <code>GrLivArea</code> is the most important feature. Congratulations on completing Chapter 2!</p>

### Fine-tuning {.unnumbered}

<p class="chapter__description">
    This chapter will teach you how to make your XGBoost models as performant as possible. You'll learn about the variety of parameters that can be adjusted to alter the behavior of XGBoost and how to tune them efficiently so that you can supercharge the performance of your models.
  </p>

#### Tuning {.unnumbered}



##### When is tuning your model a bad idea? {.unnumbered}

<p>Now that you've seen the effect that tuning has on the overall performance of your XGBoost model, let's turn the question on its head and see if you can figure out when tuning your model might not be the best idea. <strong>Given that model tuning can be time-intensive and complicated, which of the following scenarios would NOT call for careful tuning of your model</strong>?</p>

<li>You have lots of examples from some dataset and very many features at your disposal.</li>
<strong><li>You are very short on time before you must push an initial model to production and have little data to train your model on.</li></strong>
<li>You have access to a multi-core (64 cores) server with lots of memory (200GB RAM) and no time constraints.</li>
<li>You must squeeze out every last bit of performance out of your xgboost model.</li>

<p class="dc-completion-pane__message dc-u-maxw-100pc">Yup! You cannot tune if you do not have time!</p>

##### Tuning the number of boosting rounds {.unnumbered}


<div class>
<p>Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use <code>xgb.cv()</code> inside a <code>for</code> loop and build one model per <code>num_boost_round</code> parameter.</p>
<p>Here, you'll continue working with the Ames housing dataset. The features are available in the array <code>X</code>, and the target vector is contained in <code>y</code>.</p>
</div>

<li>Create a <code>DMatrix</code> called <code>housing_dmatrix</code> from <code>X</code> and <code>y</code>.</li>


<li>Create a parameter dictionary called <code>params</code>, passing in the appropriate <code>"objective"</code> (<code>"reg:linear"</code>) and <code>"max_depth"</code> (set it to <code>3</code>).</li>


<li>Iterate over <code>num_rounds</code> inside a <code>for</code> loop and perform 3-fold cross-validation. In each iteration of the loop, pass in the current number of boosting rounds (<code>curr_num_rounds</code>) to <code>xgb.cv()</code> as the argument to <code>num_boost_round</code>. </li>


<li>Append the final boosting round RMSE for each cross-validated XGBoost model to the <code>final_rmse_per_round</code> list.</li>


<li>
<code>num_rounds</code> and <code>final_rmse_per_round</code> have been zipped and converted into a DataFrame so you can easily see how the model performs with each boosting round. Hit 'Submit Answer' to see the results!</li>
```{python}
# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary for each tree: params 
params = {"objective":"reg:linear", "max_depth":3}

# Create list of number of boosting rounds
num_rounds = [5, 10, 15]

# Empty list to store final round rmse per XGBoost model
final_rmse_per_round = []

# Iterate over num_rounds and build one model per num_boost_round parameter
for curr_num_rounds in num_rounds:

    # Perform cross-validation: cv_results
    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics="rmse", as_pandas=True, seed=123)
    
    # Append final round RMSE
    final_rmse_per_round.append(cv_results["test-rmse-mean"].tail().values[-1])
    
# Print the resultant DataFrame
num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))
print(pd.DataFrame(num_rounds_rmses,columns=["num_boosting_rounds","rmse"]))
```

<p class="">Awesome! As you can see, increasing the number of boosting rounds decreases the RMSE.</p>


##### Automated boosting round selection using early_stopping {.unnumbered}


<div class>
<p>Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within <code>xgb.cv()</code>. This is done using a technique called <strong>early stopping</strong>. </p>
<p><strong>Early stopping</strong> works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (<code>"rmse"</code> in our case) does not improve for a given number of rounds. Here you will use the <code>early_stopping_rounds</code> parameter in <code>xgb.cv()</code> with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when <code>num_boost_rounds</code> is reached, then early stopping does not occur.</p>
<p>Here, the <code>DMatrix</code> and parameter dictionary have been created for you. Your task is to use cross-validation with early stopping. Go for it!</p>
</div>

<li>Perform 3-fold cross-validation with early stopping and <code>"rmse"</code> as your metric. Use <code>10</code> early stopping rounds and <code>50</code> boosting rounds. Specify a <code>seed</code> of <code>123</code> and make sure the output is a <code>pandas</code> DataFrame. Remember to specify the other parameters such as <code>dtrain</code>, <code>params</code>, and <code>metrics</code>.</li>


<li>Print <code>cv_results</code>.</li>
```{python}
# Create your housing DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary for each tree: params
params = {"objective":"reg:linear", "max_depth":4}

# Perform cross-validation with early stopping: cv_results
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=50, early_stopping_rounds=10, metrics="rmse", as_pandas=True, seed=123)

# Print cv_results
print(cv_results)
```

<p class="">Great work!</p>


#### XGBoost's hyperparameters {.unnumbered}



##### Tuning eta {.unnumbered}


<div class>
<p>It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the <code>"eta"</code>, also known as the learning rate.</p>
<p>The learning rate in XGBoost is a parameter that can range between <code>0</code> and <code>1</code>, with higher values of <code>"eta"</code> penalizing feature weights more strongly, causing much stronger regularization.</p>
</div>

<li>Create a list called <code>eta_vals</code> to store the following <code>"eta"</code> values: <code>0.001</code>, <code>0.01</code>, and <code>0.1</code>.</li>


<li>Iterate over your <code>eta_vals</code> list using a <code>for</code> loop.</li>


<li>In each iteration of the <code>for</code> loop, set the <code>"eta"</code> key of <code>params</code> to be equal to <code>curr_val</code>. Then, perform 3-fold cross-validation with early stopping (<code>5</code> rounds), <code>10</code> boosting rounds, a metric of <code>"rmse"</code>, and a <code>seed</code> of <code>123</code>. Ensure the output is a DataFrame.</li>


<li>Append the final round RMSE to the <code>best_rmse</code> list.</li>
```{python}
# Create your housing DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary for each tree (boosting round)
params = {"objective":"reg:linear", "max_depth":3}

# Create list of eta values and empty list to store final round rmse per xgboost model
eta_vals = [0.001, 0.01, 0.1]
best_rmse = []

# Systematically vary the eta
for curr_val in eta_vals:

    params["eta"] = curr_val
    
    # Perform cross-validation: cv_results
    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,
                        num_boost_round=10, early_stopping_rounds=5,
                        metrics="rmse", as_pandas=True, seed=123)
    
    # Append the final round rmse to best_rmse
    best_rmse.append(cv_results["test-rmse-mean"].tail().values[-1])

# Print the resultant DataFrame
print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=["eta","best_rmse"]))
```

<p class="">Great work!</p>


##### Tuning max_depth {.unnumbered}


<div class><p>In this exercise, your job is to tune <code>max_depth</code>, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees.</p></div>

<li>Create a list called <code>max_depths</code> to store the following <code>"max_depth"</code> values: <code>2</code>, <code>5</code>, <code>10</code>, and <code>20</code>.</li>


<li>Iterate over your <code>max_depths</code> list using a <code>for</code> loop.</li>


<li>Systematically vary <code>"max_depth"</code> in each iteration of the <code>for</code> loop and perform 2-fold cross-validation with early stopping (<code>5</code> rounds), <code>10</code> boosting rounds, a metric of <code>"rmse"</code>, and a <code>seed</code> of <code>123</code>. Ensure the output is a DataFrame.</li>
```{python}
# Create your housing DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary
params = {"objective":"reg:linear"}

# Create list of max_depth values
max_depths = [2, 5, 10, 20]
best_rmse = []

# Systematically vary the max_depth
for curr_val in max_depths:

    params["max_depth"] = curr_val
    
    # Perform cross-validation
    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,
                 num_boost_round=10, early_stopping_rounds=5,
                 metrics="rmse", as_pandas=True, seed=123)
    
    # Append the final round rmse to best_rmse
    best_rmse.append(cv_results["test-rmse-mean"].tail().values[-1])

# Print the resultant DataFrame
print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=["max_depth","best_rmse"]))
```

<p class="">Great work!</p>


##### Tuning colsample_bytree {.unnumbered}


<div class><p>Now, it's time to tune <code>"colsample_bytree"</code>. You've already seen this if you've ever worked with scikit-learn's <code>RandomForestClassifier</code> or <code>RandomForestRegressor</code>, where it just was called <code>max_features</code>. In both <code>xgboost</code> and <code>sklearn</code>, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In <code>xgboost</code>, <code>colsample_bytree</code> must be specified as a float between 0 and 1.</p></div>

<li>Create a list called <code>colsample_bytree_vals</code> to store the values <code>0.1</code>, <code>0.5</code>, <code>0.8</code>, and <code>1</code>.</li>


<li>Systematically vary <code>"colsample_bytree"</code> and perform cross-validation, exactly as you did with <code>max_depth</code> and <code>eta</code> previously.</li>
```{python}
# Create your housing DMatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary
params={"objective":"reg:linear","max_depth":3}

# Create list of hyperparameter values
colsample_bytree_vals = [0.1, 0.5, 0.8, 1]
best_rmse = []

# Systematically vary the hyperparameter value 
for curr_val in colsample_bytree_vals:

    params["colsample_bytree"] = curr_val
    
    # Perform cross-validation
    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,
                 num_boost_round=10, early_stopping_rounds=5,
                 metrics="rmse", as_pandas=True, seed=123)
    
    # Append the final round rmse to best_rmse
    best_rmse.append(cv_results["test-rmse-mean"].tail().values[-1])

# Print the resultant DataFrame
print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=["colsample_bytree","best_rmse"]))
```

<p class="">Awesome! There are several other individual parameters that you can tune, such as <code>"subsample"</code>, which dictates the fraction of the training data that is used during any given boosting round. Next up: Grid Search and Random Search to tune XGBoost hyperparameters more efficiently!</p>


#### Grid search & random search {.unnumbered}



##### Grid search with XGBoost {.unnumbered}


<div class><p>Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's <code>GridSearch</code> and <code>RandomizedSearch</code> capabilities with internal cross-validation using the <code>GridSearchCV</code> and <code>RandomizedSearchCV</code> functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let's get to work, starting with <code>GridSearchCV</code>!</p></div>


<li>Create a parameter grid called <code>gbm_param_grid</code> that contains a list of <code>"colsample_bytree"</code> values (<code>0.3</code>, <code>0.7</code>), a list with a single value for <code>"n_estimators"</code> (<code>50</code>), and a list of 2 <code>"max_depth"</code> (<code>2</code>, <code>5</code>) values.</li>


<li>Instantiate an <code>XGBRegressor</code> object called <code>gbm</code>.</li>


<li>Create a <code>GridSearchCV</code> object called <code>grid_mse</code>, passing in: the parameter grid to <code>param_grid</code>, the <code>XGBRegressor</code> to <code>estimator</code>, <code>"neg_mean_squared_error"</code> to <code>scoring</code>, and <code>4</code> to <code>cv</code>. Also specify <code>verbose=1</code> so you can better understand the output.</li>


<li>Fit the <code>GridSearchCV</code> object to <code>X</code> and <code>y</code>.</li>


<li>Print the best parameter values and lowest RMSE, using the <code>.best_params_</code> and <code>.best_score_</code> attributes, respectively, of <code>grid_mse</code>.</li>
```{python}
# edited/added
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

# Create the parameter grid: gbm_param_grid
gbm_param_grid = {
    'colsample_bytree': [0.3, 0.7],
    'n_estimators': [50],
    'max_depth': [2, 5]
}

# Instantiate the regressor: gbm
gbm = xgb.XGBRegressor()

# Perform grid search: grid_mse
grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,
                        scoring='neg_mean_squared_error', cv=4, verbose=1)
grid_mse.fit(X, y)

# Print the best parameters and lowest RMSE
print("Best parameters found: ", grid_mse.best_params_)
print("Lowest RMSE found: ", np.sqrt(np.abs(grid_mse.best_score_)))
```

<p class="">Excellent work! Next up, <code>RandomizedSearchCV</code>.</p>


##### Random search with XGBoost {.unnumbered}


<div class><p>Often, <code>GridSearchCV</code> can be really time consuming, so in practice, you may want to use <code>RandomizedSearchCV</code> instead, as you will do in this exercise. The good news is you only have to make a few modifications to your <code>GridSearchCV</code> code to do <code>RandomizedSearchCV</code>. The key difference is you have to specify a <code>param_distributions</code> parameter instead of a <code>param_grid</code> parameter.</p></div>

<li>Create a parameter grid called <code>gbm_param_grid</code> that contains a list with a single value for <code>'n_estimators'</code> (<code>25</code>), and a list of <code>'max_depth'</code> values between <code>2</code> and <code>11</code> for <code>'max_depth'</code> - use <code>range(2, 12)</code> for this. </li>


<li>Create a <code>RandomizedSearchCV</code> object called <code>randomized_mse</code>, passing in: the parameter grid to <code>param_distributions</code>, the <code>XGBRegressor</code> to <code>estimator</code>, <code>"neg_mean_squared_error"</code> to <code>scoring</code>, <code>5</code> to <code>n_iter</code>, and <code>4</code> to <code>cv</code>. Also specify <code>verbose=1</code> so you can better understand the output.</li>


<li>Fit the <code>RandomizedSearchCV</code> object to <code>X</code> and <code>y</code>.</li>
```{python}
# Create the parameter grid: gbm_param_grid 
gbm_param_grid = {
    'n_estimators': [25],
    'max_depth': range(2, 12)
}

# Instantiate the regressor: gbm
gbm = xgb.XGBRegressor(n_estimators=10)

# Perform random search: grid_mse
randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,
                                    n_iter=5, scoring='neg_mean_squared_error', cv=4, verbose=1)
randomized_mse.fit(X, y)

# Print the best parameters and lowest RMSE
print("Best parameters found: ",randomized_mse.best_params_)
print("Lowest RMSE found: ", np.sqrt(np.abs(randomized_mse.best_score_)))
```

<p class="">Superb!</p>


#### Limits {.unnumbered}



##### When should you use grid search and random search? {.unnumbered}

<p>Now that you've seen some of the drawbacks of grid search and random search, which of the following most accurately describes why both random search and grid search are non-ideal search hyperparameter tuning strategies in all scenarios?</p>

<li>Grid Search and Random Search both take a very long time to perform, regardless of the number of parameters you want to tune.</li>
<li>Grid Search and Random Search both scale exponentially in the number of hyperparameters you want to tune.</li>
<strong><li>The search space size can be massive for Grid Search in certain cases, whereas for Random Search the number of hyperparameters has a significant effect on how long it takes to run.</li></strong>
<li>Grid Search and Random Search require that you have some idea of where the ideal values for hyperparameters reside.</li>

<p class="dc-completion-pane__message dc-u-maxw-100pc">This is why random search and grid search should not always be used. Nice!</p>

### Pipelines {.unnumbered}

<p class="chapter__description">
    Take your XGBoost skills to the next level by incorporating your models into two end-to-end machine learning pipelines. You'll learn how to tune the most important XGBoost hyperparameters efficiently within a pipeline, and get an introduction to some more advanced preprocessing techniques.
  </p>

#### Review {.unnumbered}



##### Exploratory data analysis {.unnumbered}


<div class>
<p>Before diving into the nitty gritty of pipelines and preprocessing, let's do some exploratory analysis of the original, unprocessed <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">Ames housing dataset</a>. When you worked with this data in previous chapters, we preprocessed it for you so you could focus on the core XGBoost concepts. In this chapter, you'll do the preprocessing yourself!</p>
<p>A smaller version of this original, unprocessed dataset has been pre-loaded into a <code>pandas</code> DataFrame called <code>df</code>. Your task is to explore <code>df</code> in the Shell and pick the option that is <strong>incorrect</strong>. The larger purpose of this exercise is to understand the kinds of transformations you will need to perform in order to be able to use XGBoost.</p>
</div>

<li>The DataFrame has 21 columns and 1460 rows.</li>
<li>The mean of the <code>LotArea</code> column is <code>10516.828082</code>.</li>
<li>The DataFrame has missing values.</li>
<strong><li>The <code>LotFrontage</code> column has no missing values and its entries are of type <code>float64</code>.</li></strong>
<li>The standard deviation of <code>SalePrice</code> is <code>79442.502883</code>.</li>

<p class="">Well done! The <code>LotFrontage</code> column actually does have missing values: 259, to be precise. Additionally, notice how columns such as <code>MSZoning</code>, <code>PavedDrive</code>, and <code>HouseStyle</code> are categorical. These need to be encoded numerically before you can use XGBoost. This is what you'll do in the coming exercises.</p>

##### Encoding categorical columns I: LabelEncoder {.unnumbered}


<div class>
<p>Now that you've seen what will need to be done to get the housing data ready for XGBoost, let's go through the process step-by-step. </p>
<p>First, you will need to fill in missing values - as you saw previously, the column <code>LotFrontage</code> has many missing values. Then, you will need to encode any categorical columns in the dataset using one-hot encoding so that they are encoded numerically. You can watch <a href="https://campus.datacamp.com/courses/supervised-learning-with-scikit-learn/preprocessing-and-pipelines?ex=1">this video</a> from <a href="https://www.datacamp.com/courses/supervised-learning-with-scikit-learn">Supervised Learning with scikit-learn</a> for a refresher on the idea. </p>
<p>The data has five categorical columns: <code>MSZoning</code>, <code>PavedDrive</code>, <code>Neighborhood</code>, <code>BldgType</code>, and <code>HouseStyle</code>. Scikit-learn has a <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html">LabelEncoder</a> function that converts the values in each categorical column into integers. You'll practice using this here.</p>
</div>


<li>Import <code>LabelEncoder</code> from <code>sklearn.preprocessing</code>.</li>


<li>Fill in missing values in the <code>LotFrontage</code> column with <code>0</code> using <code>.fillna()</code>.</li>


<li>Create a boolean mask for categorical columns. You can do this by checking for whether <code>df.dtypes</code> equals <code>object</code>.</li>


<li>Create a <code>LabelEncoder</code> object. You can do this in the same way you instantiate any scikit-learn estimator. </li>


<li>Encode all of the categorical columns into integers using <code>LabelEncoder()</code>. To do this, use the <code>.fit_transform()</code> method of <code>le</code> in the provided lambda function.</li>
```{python}
# edited/added
df = pd.read_csv("archive/Extreme-Gradient-Boosting-with-XGBoost/datasets/ames_unprocessed_data.csv")

# Import LabelEncoder
from sklearn.preprocessing import LabelEncoder

# Fill missing values with 0
df.LotFrontage = df.LotFrontage.fillna(0)

# Create a boolean mask for categorical columns
categorical_mask = (df.dtypes == object)

# Get list of categorical column names
categorical_columns = df.columns[categorical_mask].tolist()

# Print the head of the categorical columns
print(df[categorical_columns].head())

# Create LabelEncoder object: le
le = LabelEncoder()

# Apply LabelEncoder to categorical columns
df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))

# Print the head of the LabelEncoded categorical columns
print(df[categorical_columns].head())
```

<p class="">Well done! Notice how the entries in each categorical column are now encoded numerically. A <code>BldgTpe</code> of <code>1Fam</code> is encoded as <code>0</code>, while a <code>HouseStyle</code> of <code>2Story</code> is encoded as <code>5</code>.</p>


##### Encoding categorical columns II: OneHotEncoder {.unnumbered}


<div class>
<p>Okay - so you have your categorical columns encoded numerically. Can you now move onto using pipelines and XGBoost? Not yet! In the categorical columns of this dataset, there is no natural ordering between the entries. As an example: Using <code>LabelEncoder</code>, the <code>CollgCr</code> <code>Neighborhood</code> was encoded as <code>5</code>, while the <code>Veenker</code> <code>Neighborhood</code> was encoded as <code>24</code>, and <code>Crawfor</code> as <code>6</code>. Is <code>Veenker</code> "greater" than <code>Crawfor</code> and <code>CollgCr</code>? No - and allowing the model to assume this natural ordering may result in poor performance.</p>
<p>As a result, there is another step needed: You have to apply a one-hot encoding to create binary, or "dummy" variables. You can do this using scikit-learn's <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">OneHotEncoder</a>.</p>
</div>

<li>Import <code>OneHotEncoder</code> from <code>sklearn.preprocessing</code>.</li>


<li>Instantiate a <code>OneHotEncoder</code> object called <code>ohe</code>. Specify the keyword arguments <code>categorical_features=categorical_mask</code> and <code>sparse=False</code>.</li>


<li>Using its <code>.fit_transform()</code> method, apply the <code>OneHotEncoder</code> to <code>df</code> and save the result as <code>df_encoded</code>. The output will be a NumPy array.</li>


<li>Print the first 5 rows of <code>df_encoded</code>, and then the shape of <code>df</code> as well as <code>df_encoded</code> to compare the difference.</li>
```{python}
# Import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder

# Create OneHotEncoder: ohe
ohe = OneHotEncoder(categories="auto", sparse=False)

# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded
df_encoded = ohe.fit_transform(df)

# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe
print(df_encoded[:5, :])

# Print the shape of the original DataFrame
print(df.shape)

# Print the shape of the transformed array
print(df_encoded.shape)
```

<p class="">Superb! As you can see, after one hot encoding, which creates binary variables out of the categorical variables, there are now 62 columns.</p>


##### Encoding categorical columns III: DictVectorizer {.unnumbered}


<div class>
<p>Alright, one final trick before you dive into pipelines. The two step process you just went through - <code>LabelEncoder</code> followed by <code>OneHotEncoder</code> - can be simplified by using a <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html">DictVectorizer</a>. </p>
<p>Using a <code>DictVectorizer</code> on a DataFrame that has been converted to a dictionary allows you to get label encoding as well as one-hot encoding in one go. </p>
<p>Your task is to work through this strategy in this exercise!</p>
</div>

<li>Import <code>DictVectorizer</code> from <code>sklearn.feature_extraction</code>.</li>


<li>Convert <code>df</code> into a dictionary called <code>df_dict</code> using its <code>.to_dict()</code> method with <code>"records"</code> as the argument.</li>


<li>Instantiate a <code>DictVectorizer</code> object called <code>dv</code> with the keyword argument <code>sparse=False</code>.</li>


<li>Apply the <code>DictVectorizer</code> on <code>df_dict</code> by using its <code>.fit_transform()</code> method.</li>


<li>Hit 'Submit Answer' to print the resulting first five rows and the vocabulary.</li>
```{python}
# Import DictVectorizer
from sklearn.feature_extraction import DictVectorizer

# Convert df into a dictionary: df_dict
df_dict = df.to_dict("records")

# Create the DictVectorizer object: dv
dv = DictVectorizer(sparse=False)

# Apply dv on df: df_encoded
df_encoded = dv.fit_transform(df_dict)

# Print the resulting first five rows
print(df_encoded[:5,:])

# Print the vocabulary
print(dv.vocabulary_)
```

<p class="">Fantastic! Besides simplifying the process into one step, <code>DictVectorizer</code> has useful attributes such as <code>vocabulary_</code> which maps the names of the features to their indices. With the data preprocessed, it's time to move onto pipelines!</p>


##### Preprocessing within a pipeline {.unnumbered}


<div class><p>Now that you've seen what steps need to be taken individually to properly process the Ames housing data, let's use the much cleaner and more succinct <code>DictVectorizer</code> approach and put it alongside an <code>XGBoostRegressor</code> inside of a scikit-learn pipeline.</p></div>

<li>Import <code>DictVectorizer</code> from <code>sklearn.feature_extraction</code> and <code>Pipeline</code> from <code>sklearn.pipeline</code>.</li>


<li>Fill in any missing values in the <code>LotFrontage</code> column of <code>X</code> with <code>0</code>.</li>


<li>Complete the steps of the pipeline with <code>DictVectorizer(sparse=False)</code> for <code>"ohe_onestep"</code> and <code>xgb.XGBRegressor()</code> for <code>"xgb_model"</code>.</li>


<li>Create the pipeline using <code>Pipeline()</code> and <code>steps</code>.</li>


<li>Fit the <code>Pipeline</code>. Don't forget to convert <code>X</code> into a format that <code>DictVectorizer</code> understands by calling the <code>to_dict("records")</code> method on <code>X</code>.</li>
```{python}
# Import necessary modules
from sklearn.feature_extraction import DictVectorizer
from sklearn.pipeline import Pipeline

# Fill LotFrontage missing values with 0
X.LotFrontage = X.LotFrontage.fillna(0)

# Setup the pipeline steps: steps
steps = [("ohe_onestep", DictVectorizer(sparse=False)),
         ("xgb_model", xgb.XGBRegressor())]
         
# Create the pipeline: xgb_pipeline
xgb_pipeline = Pipeline(steps)

# Fit the pipeline
xgb_pipeline.fit(X.to_dict("records"), y)
```

<p class="">Well done! It's now time to see what it takes to use XGBoost within pipelines.</p>


#### Incorporating XGBoost {.unnumbered}



##### Cross-validating your XGBoost model {.unnumbered}


<div class><p>In this exercise, you'll go one step further by using the pipeline you've created to preprocess <strong>and</strong> cross-validate your model.</p></div>

<li>Create a pipeline called <code>xgb_pipeline</code> using <code>steps</code>.</li>


<li>Perform 10-fold cross-validation using <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"><code>cross_val_score()</code></a>. You'll have to pass in the pipeline, <code>X</code> (as a dictionary, using <code>.to_dict("records")</code>), <code>y</code>, the number of folds you want to use, and <code>scoring</code> (<code>"neg_mean_squared_error"</code>). </li>


<li>Print the 10-fold RMSE.</li>
```{python}
# Import necessary modules
from sklearn.feature_extraction import DictVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

# Fill LotFrontage missing values with 0
X.LotFrontage = X.LotFrontage.fillna(0)

# Setup the pipeline steps: steps
steps = [("ohe_onestep", DictVectorizer(sparse=False)),
         ("xgb_model", xgb.XGBRegressor(max_depth=2, objective="reg:linear"))]

# Create the pipeline: xgb_pipeline
xgb_pipeline = Pipeline(steps)

# Cross-validate the model
cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict("records"), y, cv=10, scoring="neg_mean_squared_error")

# Print the 10-fold RMSE
print("10-fold RMSE: ", np.mean(np.sqrt(np.abs(cross_val_scores))))
```

<p class="">Great work!</p>


##### Kidney disease case study I: Categorical Imputer {.unnumbered}


<div class>
<p>You'll now continue your exploration of using pipelines with a dataset that requires significantly more wrangling. The <a href="https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease">chronic kidney disease dataset</a> contains both categorical and numeric features, but contains lots of missing values. The goal here is to predict who has chronic kidney disease given various blood indicators as features.</p>
<p>As Sergey mentioned in the video, you'll be introduced to a new library, <a href="https://github.com/pandas-dev/sklearn-pandas"><code>sklearn_pandas</code></a>, that allows you to chain many more processing steps inside of a pipeline than are currently supported in scikit-learn. Specifically, you'll be able to impute missing categorical values directly using the <code>Categorical_Imputer()</code> class in <code>sklearn_pandas</code>, and the <code>DataFrameMapper()</code> class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.</p>
<p>We've also created a transformer called a <code>Dictifier</code> that encapsulates converting a DataFrame using <code>.to_dict("records")</code> without you having to do it explicitly (and so that it works in a pipeline). Finally, we've also provided the list of feature names in <code>kidney_feature_names</code>, the target name in <code>kidney_target_name</code>, the features in <code>X</code>, and the target in <code>y</code>.</p>
<p>In this exercise, your task is to apply the <code>CategoricalImputer</code> to impute all of the categorical columns in the dataset. You can refer to how the numeric imputation mapper was created as a template. Notice the keyword arguments <code>input_df=True</code> and <code>df_out=True</code>? This is so that you can work with DataFrames instead of arrays. By default, the transformers are passed a <code>numpy</code> array of the selected columns as input, and as a result, the output of the DataFrame mapper is also an array. Scikit-learn transformers have historically been designed to work with <code>numpy</code> arrays, not <code>pandas</code> DataFrames, even though their basic indexing interfaces are similar.</p>
</div>


<li>Apply the categorical imputer using <code>DataFrameMapper()</code> and <code>SimpleImputer()</code>. <code>SimpleImputer()</code> does not need any arguments to be passed in. The columns are contained in <code>categorical_columns</code>. Be sure to specify <code>input_df=True</code> and <code>df_out=True</code>, and use <code>category_feature</code> as your iterator variable in the list comprehension.</li>
```{python}
# edited/added
import pandas as pd
X = pd.read_csv('archive/Extreme-Gradient-Boosting-with-XGBoost/datasets/chronic_kidney_X.csv')
y = pd.read_csv('archive/Extreme-Gradient-Boosting-with-XGBoost/datasets/chronic_kidney_y.csv').to_numpy().ravel()

# Import necessary modules
from sklearn_pandas import DataFrameMapper, CategoricalImputer
from sklearn.impute import SimpleImputer

# Check number of nulls in each feature columns
nulls_per_column = X.isnull().sum()
print(nulls_per_column)

# Create a boolean mask for categorical columns
categorical_feature_mask = X.dtypes == object

# Get list of categorical column names
categorical_columns = X.columns[categorical_feature_mask].tolist()

# Get list of non-categorical column names
non_categorical_columns = X.columns[~categorical_feature_mask].tolist()

# Apply numeric imputer
numeric_imputation_mapper = DataFrameMapper(
    [([numeric_feature], SimpleImputer(strategy='median')) 
     for numeric_feature in non_categorical_columns],
    input_df=True,
    df_out=True
)

# Apply categorical imputer
categorical_imputation_mapper = DataFrameMapper(
    [(category_feature, CategoricalImputer()) 
     for category_feature in categorical_columns],
    input_df=True,
    df_out=True
)
```

<p class="">Great work!</p>


##### Kidney disease case study II: Feature Union {.unnumbered}


<div class>
<p>Having separately imputed numeric as well as categorical columns, your task is now to use scikit-learn's <a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html">FeatureUnion</a> to concatenate their results, which are contained in two separate transformer objects - <code>numeric_imputation_mapper</code>, and <code>categorical_imputation_mapper</code>, respectively.</p>
<p>You may have already encountered <code>FeatureUnion</code> in <a href="https://campus.datacamp.com/courses/machine-learning-with-the-experts-school-budgets/improving-your-model?ex=7">Machine Learning with the Experts: School Budgets</a>. Just like with pipelines, you have to pass it a list of <code>(string, transformer)</code> tuples, where the first half of each tuple is the name of the transformer.</p>
</div>

<li>Import <code>FeatureUnion</code> from <code>sklearn.pipeline</code>.</li>


<li>Combine the results of <code>numeric_imputation_mapper</code> and <code>categorical_imputation_mapper</code> using <code>FeatureUnion()</code>, with the names <code>"num_mapper"</code> and <code>"cat_mapper"</code> respectively.</li>
```{python}
# Import FeatureUnion
from sklearn.pipeline import FeatureUnion

# Combine the numeric and categorical transformations
numeric_categorical_union = FeatureUnion([
                                          ("num_mapper", numeric_imputation_mapper),
                                          ("cat_mapper", categorical_imputation_mapper)
                                         ])
```

<p class="">Great work!</p>


##### Kidney disease case study III: Full pipeline {.unnumbered}


<div class>
<p>It's time to piece together all of the transforms along with an <code>XGBClassifier</code> to build the full pipeline!</p>
<p>Besides the <code>numeric_categorical_union</code> that you created in the previous exercise, there are two other transforms needed: the <code>Dictifier()</code> transform which we created for you, and the <code>DictVectorizer()</code>. </p>
<p>After creating the pipeline, your task is to cross-validate it to see how well it performs.</p>
</div>


<li>Create the pipeline using the <code>numeric_categorical_union</code>, <code>Dictifier()</code>, and <code>DictVectorizer(sort=False)</code> transforms, and <code>xgb.XGBClassifier()</code> estimator with <code>max_depth=3</code>. Name the transforms <code>"featureunion"</code>, <code>"dictifier"</code> <code>"vectorizer"</code>, and the estimator <code>"clf"</code>.</li>


<li>Perform 3-fold cross-validation on the <code>pipeline</code> using <code>cross_val_score()</code>. Pass it the pipeline, <code>pipeline</code>, the features, <code>kidney_data</code>, the outcomes, <code>y</code>. Also set <code>scoring</code> to <code>"roc_auc"</code> and <code>cv</code> to <code>3</code>.</li>
```{python}
# edited/added
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.feature_extraction import DictVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
import xgboost as xgb
import numpy as np

# Define Dictifier class to turn df into dictionary as part of pipeline
class Dictifier(BaseEstimator, TransformerMixin):       
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        if type(X) == pd.core.frame.DataFrame:
            return X.to_dict("records")
        else:
            return pd.DataFrame(X).to_dict("records")
          
# Create full pipeline
pipeline = Pipeline([
                     ("featureunion", numeric_categorical_union),
                     ("dictifier", Dictifier()),
                     ("vectorizer", DictVectorizer(sort=False)),
                     ("clf", xgb.XGBClassifier(max_depth=3))
                    ])
                    
# Perform cross-validation
cross_val_scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=3)

# Print avg. AUC
print("3-fold AUC: ", np.mean(cross_val_scores))
```

<p class="">Great work!</p>


#### Tuning XGBoost {.unnumbered}



##### Bringing it all together {.unnumbered}


<div class>
<p>Alright, it's time to bring together everything you've learned so far! In this final exercise of the course, you will combine your work from the previous exercises into one end-to-end XGBoost pipeline to really cement your understanding of preprocessing and pipelines in XGBoost.</p>
<p>Your work from the previous 3 exercises, where you preprocessed the data and set up your pipeline, has been pre-loaded. Your job is to perform a randomized search and identify the best hyperparameters.</p>
</div>

<li>Set up the parameter grid to tune <code>'clf__learning_rate'</code> (from <code>0.05</code> to <code>1</code> in increments of <code>0.05</code>), <code>'clf__max_depth'</code> (from <code>3</code> to <code>10</code> in increments of <code>1</code>), and <code>'clf__n_estimators'</code> (from <code>50</code> to <code>200</code> in increments of <code>50</code>).</li>


<li>Using your <code>pipeline</code> as the estimator, perform 2-fold <code>RandomizedSearchCV</code> with an <code>n_iter</code> of <code>2</code>. Use <code>"roc_auc"</code> as the metric, and set <code>verbose</code> to <code>1</code> so the output is more detailed. Store the result in <code>randomized_roc_auc</code>.</li>


<li>Fit <code>randomized_roc_auc</code> to <code>X</code> and <code>y</code>.</li>


<li>Compute the best score and best estimator of <code>randomized_roc_auc</code>.</li>
```{python}
# edited/added
from sklearn.model_selection import RandomizedSearchCV

# Create the parameter grid
gbm_param_grid = {
    'clf__learning_rate': np.arange(.05, 1, .05),
    'clf__max_depth': np.arange(3,10, 1),
    'clf__n_estimators': np.arange(50, 200, 50)
}

# Perform RandomizedSearchCV
randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,
                                        param_distributions=gbm_param_grid,
                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)
                                        
# Fit the estimator
randomized_roc_auc.fit(X, y)

# Compute metrics
print(randomized_roc_auc.best_score_)
print(randomized_roc_auc.best_estimator_)
```

<p class="">Amazing work! This type of pipelining is very common in real-world data science and you're well on your way towards mastering it.</p>


#### Final Thoughts {.unnumbered}

##### Final Thoughts {.unnumbered}
Congratulations on completing this course. Let's go over everything we've covered in this course, as well as where you can go from here with learning other topics related to XGBoost that we didn't have a chance to cover.

##### What We Have Covered And You Have Learned {.unnumbered}

So, what have we been able to cover in this course? Well, we've learned how to use XGBoost for both classification and regression tasks. We've also covered all the most important hyperparameters that you should tune when creating XGBoost models, so that they are as performant as possible. And we just finished up how to incorporate XGBoost into pipelines, and used some more advanced functions that allow us to seamlessly work with Pandas DataFrames and scikit-learn. That's quite a lot of ground we've covered and you should be proud of what you've been able to accomplish.

##### What We Have Not Covered (And How You Can Proceed) {.unnumbered}

However, although we've covered quite a lot, we didn't cover some other topics that would advance your mastery of XGBoost. Specifically, we never looked into how to use XGBoost for ranking or recommendation problems, which can be done by modifying the loss function you use when constructing your model. We also didn't look into more advanced hyperparameter selection strategies. The most powerful strategy, called Bayesian optimization, has been used with lots of success, and entire companies have been created just for specifically using this method in tuning models (for example, the company sigopt does exactly this). It's a powerful method, but would take an entire other DataCamp course to teach properly! Finally, we haven't talked about ensembling XGBoost with other models. Although XGBoost is itself an ensemble method, nothing stops you from combining the predictions you get from an XGBoost model with other models, as this is usually a very powerful additional way to squeeze the last bit of juice from your data. Learning about all of these additional topics will help you become an even more powerful user of XGBoost. Now that you know your way around the package, there's no reason for you to stop learning how to get even more benefits out of it.

##### Congratulations! {.unnumbered}

I hope you've enjoyed taking this course on XGBoost as I have teaching it. Please let us know if you've enjoyed the course and definitely let me know how I can improve it. It's been a pleasure, and I hope you continue your data science journey from here!