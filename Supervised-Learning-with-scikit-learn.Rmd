## Supervised Learning {.unnumbered}

<h3 class="course__description-title">George Boorman</h3>
<p class="course__instructor-description display-none-mobile-course-page-experiment">
    George is an Analytics and Data Science Curriculum Manager at DataCamp. He holds a PGDip in Exercise for Health and BSc (Hons) in Sports Science and has experience in project management across public health, applied research, and not-for-profit sectors. George is passionate about sports, tech for good, and all things data science. 
  </p>

**Course Description**

<p class="course__description">Grow your machine learning skills with scikit-learn and discover how to use this popular Python library to train models using labeled data. In this course, you'll learn how to make powerful predictions, such as whether a customer is will churn from your business, whether an individual has diabetes, and even how to tell classify the genre of a song. Using real-world datasets, you'll find out how to build predictive models, tune their parameters, and determine how well they will perform with unseen data.</p>

### Classification {.unnumbered}

In this chapter, you'll be introduced to classification problems and learn how to solve them using supervised learning techniques. You'll learn how to split data into training and test sets, fit a model, make predictions, and evaluate accuracy. You’ll discover the relationship between model complexity and performance, applying what you learn to a churn dataset, where you will classify the churn status of a telecom company's customers.

#### Scikit-learn {.unnumbered}



##### Binary classification {.unnumbered}


<div class>
<p>In the video, you saw that there are two types of supervised learning — classification and regression. Recall that binary classification is used to predict a target variable that has only two labels, typically represented numerically with a zero or a one. </p>
<p>A dataset, <code>churn_df</code>, has been preloaded for you in the console. </p>
<p>Your task is to examine the data and choose which column could be the target variable for binary classification.</p>
</div>

```{python}
# edited/added
import pandas as pd
churn_df = pd.read_csv("archive/Supervised-Learning-with-scikit-learn/datasets/telecom_churn_clean.csv", index_col=[0])
churn_df.info()
```

- [ ] <code>"customer_service_calls"</code>
- [ ] <code>"total_night_charge"</code>
- [x] <code>"churn"</code>
- [ ] <code>"account_length"</code>

<p class="">Correct! <code>churn</code> has values of <code>0</code> or <code>1</code>, so it can be predicted using a binary classification model.</p>

##### The supervised learning workflow {.unnumbered}

<div class=""><p>Recall that scikit-learn offers a repeatable workflow for using supervised learning models to predict the target variable values when presented with new data. </p>
<p>Reorder the pseudo-code provided so it accurately represents the workflow of building a supervised learning model and making predictions.</p></div>

<li>Drag the code blocks into the correct order to represent how a supervised learning workflow would be executed.</li>

```{python, eval=F}
from sklearn.module import Model
model = Model()
model.fit(X, y)
model.predict(X_new)
```

<p>Great work! You can see how scikit-learn enables predictions to be made in only a few lines of code!</p>

#### The classification challenge {.unnumbered}



##### k-Nearest Neighbors: Fit {.unnumbered}


<div class>
<p>In this exercise, you will build your first classification model using the <code>churn_df</code> dataset, which has been preloaded for the remainder of the chapter. </p>
<p>The features to use will be <code>"account_length"</code> and <code>"customer_service_calls"</code>. The target, <code>"churn"</code>, needs to be a single column with the same number of observations as the feature data. </p>
<p>You will convert the features and the target variable into NumPy arrays, create an instance of a KNN classifier, and then fit it to the data. </p>
<p><code>numpy</code> has also been preloaded for you as <code>np</code>.</p>
</div>

<li>Import <code>KNeighborsClassifier</code> from <code>sklearn.neighbors</code>.</li>

<li>Create an array called <code>X</code> containing values from the <code>"account_length"</code> and <code>"customer_service_calls"</code> columns, and an array called <code>y</code> for the values of the <code>"churn"</code> column. </li>

<li>Instantiate a <code>KNeighborsClassifier</code> called <code>knn</code> with <code>6</code> neighbors.</li>

<li>Fit the classifier to the data using the <code>.fit()</code> method.</li>
```{python}
# Import KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier 

# Create arrays for the features and the target variable
y = churn_df["churn"].values
X = churn_df[["account_length", "customer_service_calls"]].values

# Create a KNN classifier with 6 neighbors
knn = KNeighborsClassifier(n_neighbors=6)

# Fit the classifier to the data
knn.fit(X, y)
```

<p class="">Excellent! Now that your KNN classifier has been fit to the data, it can be used to predict the labels of new data points.</p>

##### k-Nearest Neighbors: Predict {.unnumbered}


<div class>
<p>Now you have fit a KNN classifier, you can use it to predict the label of new data points. All available data was used for training, however, fortunately, there are new observations available. These have been preloaded for you as <code>X_new</code>. </p>
<p>The model <code>knn</code>, which you created and fit the data in the last exercise, has been preloaded for you. You will use your classifier to predict the labels of a set of new data points:</p>
<pre><code>X_new = np.array([[30.0, 17.5],
                  [107.0, 24.1],
                  [213.0, 10.9]])
</code></pre>
</div>


<li>Create <code>y_pred</code> by predicting the target values of the unseen features <code>X_new</code>.</li>

<li>Print the predicted labels for the set of predictions.</li>
```{python}
# edited/added
import numpy as np
X_new = np.array([[30.0, 17.5], [107.0, 24.1], [213.0, 10.9]])

# Predict the labels for the X_new
y_pred = knn.predict(X_new)

# Print the predictions for X_new
print("Predictions: {}".format(y_pred)) 
```

<p class="">Great work! The model has predicted the first and third customers will not churn in the new array. But how do we know how accurate these predictions are? Let's explore how to measure a model's performance in the next video.</p>

#### Measuring performance {.unnumbered}



##### Train/test split + computing accuracy {.unnumbered}


<div class>
<p>Now that you have learned about the importance of splitting your data into training and test sets, it's time to practice doing this on the <code>churn_df</code> dataset! </p>
<p>NumPy arrays have been created for you containing the features as <code>X</code> and the target variable as <code>y</code>. You will split them into training and test sets, fit a KNN classifier to the training data, and then compute its accuracy on the test data using the <code>.score()</code> method.</p>
</div>

<li>Import <code>train_test_split</code> from <code>sklearn.model_selection</code>.</li>

<li>Split <code>X</code> and <code>y</code> into training and test sets, setting <code>test_size</code> equal to 20%, <code>random_state</code> to <code>42</code>, and ensuring the target label proportions reflect that of the original dataset.</li>

<li>Fit the <code>knn</code> model to the training data.</li>

<li>Compute and print the model's accuracy for the test data.</li>
```{python}
# Import the module
from sklearn.model_selection import train_test_split

X = churn_df.drop("churn", axis=1).values
y = churn_df["churn"].values

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
knn = KNeighborsClassifier(n_neighbors=5)

# Fit the classifier to the training data
knn.fit(X_train, y_train)

# Print the accuracy
print(knn.score(X_test, y_test))
```

<p class="">Excellent! In a few lines of code you split a dataset, fit a KNN model, and found its accuracy to be 87%!</p>

##### Overfitting and underfitting {.unnumbered}


<div class>
<p>Interpreting model complexity is a great way to evaluate performance when utilizing supervised learning. Your aim is to produce a model that can interpret the relationship between features and the target variable, as well as generalize well when exposed to new observations. </p>
<p>You will generate accuracy scores for the training and test sets using a KNN classifier with different <code>n_neighbor</code> values, which you will plot in the next exercise. </p>
<p>The training and test sets have been created from the <code>churn_df</code> dataset and preloaded as <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code>. </p>
<p>In addition, <code>KNeighborsClassifier</code> has been imported for you along with <code>numpy</code> as <code>np</code>.</p>
</div>

<li>Create <code>neighbors</code> as a <code>numpy</code> array of values from <code>1</code> up to and including <code>12</code>.</li>

<li>Instantiate a KNN classifier, with the number of neighbors equal to the <code>neighbor</code> iterator.</li>

<li>Fit the model to the training data.</li>

<li>Calculate accuracy scores for the training set and test set separately using the <code>.score()</code> method, and assign the results to the index of the <code>train_accuracies</code> and <code>test_accuracies</code> dictionaries, respectively.</li>
```{python}
# Create neighbors
neighbors = np.arange(1, 13)
train_accuracies = {}
test_accuracies = {}

for neighbor in neighbors:
  
  	# Set up a KNN Classifier
  	knn = KNeighborsClassifier(n_neighbors=neighbor)
  
  	# Fit the model
  	knn.fit(X_train, y_train)
  
  	# Compute accuracy
  	train_accuracies[neighbor] = knn.score(X_train, y_train)
  	test_accuracies[neighbor] = knn.score(X_test, y_test)
print(neighbors, '\n', train_accuracies, '\n', test_accuracies)
```

<p class="">Notice how training accuracy decreases as the number of neighbors initially gets larger, and vice versa for the testing accuracy? These scores would be much easier to interpret in a line plot, so let's produce a model complexity curve of these results.</p>

##### Visualizing model complexity {.unnumbered}


<div class>
<p>Now you have calculated the accuracy of the KNN model on the training and test sets using various values of <code>n_neighbors</code>, you can create a model complexity curve to visualize how performance changes as the model becomes less complex!</p>
<p>The variables <code>neighbors</code>, <code>train_accuracies</code>, and <code>test_accuracies</code>, which you generated in the previous exercise, have all been preloaded for you. You will plot the results to aid in finding the optimal number of neighbors for your model.</p>
</div>

<li>Add a title <code>"KNN: Varying Number of Neighbors"</code>.</li>

<li>Plot the <code>.values()</code> method of <code>train_accuracies</code> on the y-axis against <code>neighbors</code> on the x-axis, with a label of <code>"Training Accuracy"</code>.</li>

<li>Plot the <code>.values()</code> method of <code>test_accuracies</code> on the y-axis against <code>neighbors</code> on the x-axis, with a label of <code>"Testing Accuracy"</code>.</li>

<li>Display the plot.</li>
```{python}
# edited/added
import matplotlib.pyplot as plt

# Add a title
plt.title("KNN: Varying Number of Neighbors")

# Plot training accuracies
plt.plot(neighbors, list(train_accuracies.values()), label="Training Accuracy") # edited/added

# Plot test accuracies
plt.plot(neighbors, list(test_accuracies.values()), label="Testing Accuracy") # edited/added

plt.legend()
plt.xlabel("Number of Neighbors")
plt.ylabel("Accuracy")

# Display the plot
plt.show()
```

<p class="">Great work! See how training accuracy decreases and test accuracy increases as the number of neighbors gets larger. For the test set, accuracy peaks with 7 neighbors, suggesting it is the optimal value for our model. Now let's explore regression models!</p>

### Regression {.unnumbered}

In this chapter, you will be introduced to regression, and build models to predict sales values using a dataset on advertising expenditure. You will learn about the mechanics of linear regression and common performance metrics such as R-squared and root mean squared error. You will perform k-fold cross-validation, and apply regularization to regression models to reduce the risk of overfitting.

#### Introduction to regression {.unnumbered}



##### Creating features {.unnumbered}


<div class>
<p>In this chapter, you will work with a dataset called <code>sales_df</code>, which contains information on advertising campaign expenditure across different media types, and the number of dollars generated in sales for the respective campaign. The dataset has been preloaded for you. Here are the first two rows:</p>
<pre><code>     tv        radio      social_media    sales
1    13000.0   9237.76    2409.57         46677.90
2    41000.0   15886.45   2913.41         150177.83
</code></pre>
<p>You will use the advertising expenditure as features to predict sales values, initially working with the <code>"radio"</code> column. However, before you make any predictions you will need to create the feature and target arrays, reshaping them to the correct format for scikit-learn.</p>
</div>


<li>Create <code>X</code>, an array of the values from the <code>sales_df</code> DataFrame's <code>"radio"</code> column.</li>

<li>Create <code>y</code>, an array of the values from the <code>sales_df</code> DataFrame's <code>"sales"</code> column.</li>

<li>Reshape <code>X</code> into a two-dimensional NumPy array.</li>

<li>Print the shape of <code>X</code> and <code>y</code>.</li>
```{python}
# edited/added
sales_df = pd.read_csv("archive/Supervised-Learning-with-scikit-learn/datasets/advertising_and_sales_clean.csv").drop(['influencer'], axis = 1)

import numpy as np

# Create X from the radio column's values
X = sales_df["radio"].values

# Create y from the sales column's values
y = sales_df["sales"].values

# Reshape X
X = X.reshape(-1, 1)

# Check the shape of the features and targets
print(X.shape, y.shape)
```

<p class="">Excellent! See that there are 4546 values in both arrays? Now let's build a linear regression model!</p>

##### Building a linear regression model {.unnumbered}


<div class>
<p>Now you have created your feature and target arrays, you will train a linear regression model on all feature and target values. </p>
<p>As the goal is to assess the relationship between the feature and target values there is no need to split the data into training and test sets. </p>
<p><code>X</code> and <code>y</code> have been preloaded for you as follows:</p>
<pre><code>y = sales_df["sales"].values
X = sales_df["radio"].values.reshape(-1, 1)
</code></pre>
</div>

<li>Import <code>LinearRegression</code>.</li>

<li>Instantiate a linear regression model.</li>

<li>Predict sales values using <code>X</code>, storing as <code>predictions</code>.</li>
```{python}
# Import LinearRegression
from sklearn.linear_model import LinearRegression

# Create the model
reg = LinearRegression()

# Fit the model to the data
reg.fit(X, y)

# Make predictions
predictions = reg.predict(X)

print(predictions[:5])
```

<p class="">Great model building! See how sales values for the first five predictions range from \$95,000 to over \$290,000. Let's visualize the model's fit.</p>

##### Visualizing a linear regression model {.unnumbered}


<div class>
<p>Now you have built your linear regression model and trained it using all available observations, you can visualize how well the model fits the data. This allows you to interpret the relationship between <code>radio</code> advertising expenditure and <code>sales</code> values.</p>
<p>The variables <code>X</code>, an array of <code>radio</code> values, <code>y</code>, an array of <code>sales</code> values, and <code>predictions</code>, an array of the model's predicted values for <code>y</code> given <code>X</code>, have all been preloaded for you from the previous exercise.</p>
</div>

<li>Import <code>matplotlib.pyplot</code> as <code>plt</code>.</li>

<li>Create a scatter plot visualizing <code>y</code> against <code>X</code>, with observations in blue.</li>

<li>Draw a red line plot displaying the predictions against <code>X</code>.</li>

<li>Display the plot.</li>
```{python}
# Import matplotlib.pyplot
import matplotlib.pyplot as plt

# Create scatter plot
plt.scatter(X, y, color="blue")

# Create line plot
plt.plot(X, predictions, color="red")
plt.xlabel("Radio Expenditure ($)")
plt.ylabel("Sales ($)")

# Display the plot
plt.show()
```

<p class="">The model nicely captures a near-perfect linear correlation between radio advertising expenditure and sales! Now let's take a look at what is going on under the hood to calculate this relationship.</p>

#### The basics of linear regression {.unnumbered}



##### Fit and predict for regression {.unnumbered}


<div class>
<p>Now you have seen how linear regression works, your task is to create a multiple linear regression model using all of the features in the <code>sales_df</code> dataset, which has been preloaded for you. As a reminder, here are the first two rows:</p>
<pre><code>     tv        radio      social_media    sales
1    13000.0   9237.76    2409.57         46677.90
2    41000.0   15886.45   2913.41         150177.83
</code></pre>
<p>You will then use this model to predict sales based on the values of the test features.</p>
<p><code>LinearRegression</code> and <code>train_test_split</code> have been preloaded for you from their respective modules.</p>
</div>

<li>Create <code>X</code>, an array containing values of all features in <code>sales_df</code>, and <code>y</code>, containing all values from the <code>"sales"</code> column.</li>

<li>Instantiate a linear regression model.</li>

<li>Fit the model to the training data.</li>

<li>Create <code>y_pred</code>, making predictions for <code>sales</code> using the test features.</li>
```{python}
# Create X and y arrays
X = sales_df.drop("sales", axis=1).values
y = sales_df["sales"].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Instantiate the model
reg = LinearRegression()

# Fit the model to the data
reg.fit(X_train, y_train)

# Make predictions
y_pred = reg.predict(X_test)
print("Predictions: {}, Actual Values: {}".format(y_pred[:2], y_test[:2]))
```

<p class="">Great work! The first two predictions appear to be within around 5% of the actual values from the test set!</p>

##### Regression performance {.unnumbered}


<div class>
<p>Now you have fit a model, <code>reg</code>, using all features from <code>sales_df</code>, and made predictions of sales values, you can evaluate performance using some common regression metrics. </p>
<p>The variables <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, <code>y_test</code>, and <code>y_pred</code>, along with the fitted model, <code>reg</code>, all from the last exercise, have been preloaded for you.</p>
<p>Your task is to find out how well the features can explain the variance in the target values, along with assessing the model's ability to make predictions on unseen data.</p>
</div>

<li>Import <code>mean_squared_error</code>.</li>

<li>Calculate the model's R-squared score by passing the test feature values and the test target values to an appropriate method.</li>

<li>Calculate the model's root mean squared error using <code>y_test</code> and <code>y_pred</code>.</li>

<li>Print <code>r_squared</code> and <code>rmse</code>.</li>
```{python}
# Import mean_squared_error
from sklearn.metrics import mean_squared_error

# Compute R-squared
r_squared = reg.score(X_test, y_test)

# Compute RMSE
rmse = mean_squared_error(y_test, y_pred, squared=False)

# Print the metrics
print("R^2: {}".format(r_squared))
print("RMSE: {}".format(rmse))
```

<p class="">Wow, the features explain 99.9% of the variance in sales values! Looks like this company's advertising strategy is working well!</p>

#### Cross-validation {.unnumbered}



##### Cross-validation for R-squared {.unnumbered}


<div class>
<p>Cross-validation is a vital approach to evaluating a model. It maximizes the amount of data that is available to the model, as the model is not only trained but also tested on all of the available data.</p>
<p>In this exercise, you will build a linear regression model, then use 6-fold cross-validation to assess its accuracy for predicting sales using social media advertising expenditure. You will display the individual score for each of the six-folds.</p>
<p>The <code>sales_df</code> dataset has been split into <code>y</code> for the target variable, and <code>X</code> for the features, and preloaded for you. <code>LinearRegression</code> has been imported from <code>sklearn.linear_model</code>.</p>
</div>

<li>Import <code>KFold</code> and <code>cross_val_score</code>.</li>

<li>Create <code>kf</code> by calling <code>KFold()</code>, setting the number of splits to six, <code>shuffle</code> to <code>True</code>, and setting a seed of <code>5</code>.</li>

<li>Perform cross-validation using <code>reg</code> on <code>X</code> and <code>y</code>, passing <code>kf</code> to <code>cv</code>.</li>

<li>Print the <code>cv_scores</code>.</li>
```{python}
# Import the necessary modules
from sklearn.model_selection import KFold, cross_val_score

# Create a KFold object
kf = KFold(n_splits=6, shuffle=True, random_state=5)

reg = LinearRegression()

# Compute 6-fold cross-validation scores
cv_scores = cross_val_score(reg, X, y, cv=kf)

# Print scores
print(cv_scores)
```

<p class="">Notice how R-squared for each fold ranged between <code>0.74</code> and <code>0.77</code>? By using cross-validation, we can see how performance varies depending on how the data is split!</p>

##### Analyzing cross-validation metrics {.unnumbered}


<div class>
<p>Now you have performed cross-validation, it's time to analyze the results. </p>
<p>You will display the mean, standard deviation, and 95% confidence interval for <code>cv_results</code>, which has been preloaded for you from the previous exercise.</p>
<p><code>numpy</code> has been imported for you as <code>np</code>.</p>
</div>


<li>Calculate and print the mean of the results.</li>

<li>Calculate and print the standard deviation of <code>cv_results</code>.</li>

<li>Display the 95% confidence interval for your results using <code>np.quantile()</code>.</li>
```{python}
# edited/added
cv_results = cv_scores

# Print the mean
print(np.mean(cv_results))

# Print the standard deviation
print(np.std(cv_results))

# Print the 95% confidence interval
print(np.quantile(cv_results, [0.025, 0.975]))
```

<p class="">An average score of <code>0.75</code> with a low standard deviation is pretty good for a model out of the box! Now let's learn how to apply regularization to our regression models.</p>

#### Regularized regression {.unnumbered}



##### Regularized regression: Ridge {.unnumbered}


<div class>
<p>Ridge regression performs regularization by computing the <em>squared</em> values of the model parameters multiplied by alpha and adding them to the loss function. </p>
<p>In this exercise, you will fit ridge regression models over a range of different alpha values, and print their $\(R^2\)$ scores. You will use all of the features in the <code>sales_df</code> dataset to predict <code>"sales"</code>. The data has been split into <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, <code>y_test</code> for you. </p>
<p>A variable called <code>alphas</code> has been provided as a list containing different alpha values, which you will loop through to generate scores.</p>
</div>

<li>Import <code>Ridge</code>.</li>

<li>Instantiate <code>Ridge</code>, setting alpha equal to <code>alpha</code>.</li>

<li>Fit the model to the training data.</li>

<li>Calculate the $(R^2)$ score for each iteration of <code>ridge</code>.</li>
```{python}
# Import Ridge
from sklearn.linear_model import Ridge
alphas = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]
ridge_scores = []

for alpha in alphas:
  
  # Create a Ridge regression model
  ridge = Ridge(alpha=alpha)
  
  # Fit the data
  ridge.fit(X_train, y_train)
  
  # Obtain R-squared
  score = ridge.score(X_test, y_test)
  ridge_scores.append(score)
print(ridge_scores)
```

<p class="">Well done! The scores don't appear to change much as <code>alpha</code> increases, which is indicative of how well the features explain the variance in the target—even by heavily penalizing large coefficients, underfitting does not occur!</p>

##### Lasso regression for feature importance {.unnumbered}


<div class>
<p>In the video, you saw how lasso regression can be used to identify important features in a dataset.</p>
<p>In this exercise, you will fit a lasso regression model to the <code>sales_df</code> data and plot the model's coefficients.</p>
<p>The feature and target variable arrays have been pre-loaded as <code>X</code> and <code>y</code>, along with <code>sales_columns</code>, which contains the dataset's feature names.</p>
</div>


<li>Import <code>Lasso</code> from <code>sklearn.linear_model</code>.</li>

<li>Instantiate a Lasso regressor with an alpha of <code>0.3</code>. </li>

<li>Fit the model to the data. </li>

<li>Compute the model's coefficients, storing as <code>lasso_coef</code>.</li>
```{python}
# edited/added
sales_columns = sales_df.drop(['sales'], axis = 1).columns

# Import Lasso
from sklearn.linear_model import Lasso

# Instantiate a lasso regression model
lasso = Lasso(alpha=0.3)

# Fit the model to the data
lasso.fit(X, y)

# Compute and print the coefficients
lasso_coef = lasso.coef_
print(lasso_coef)
plt.bar(sales_columns, lasso_coef)
plt.xticks(rotation=45)
plt.show()
```

<p class="">See how the figure makes it clear that expenditure on TV advertising is the most important feature in the dataset to predict sales values! In the next chapter, we will learn how to further assess and improve our model's performance!</p>

### Fine-Tuning Your Model {.unnumbered}

Having trained models, now you will learn how to evaluate them. In this chapter, you will be introduced to several metrics along with a visualization technique for analyzing classification model performance using scikit-learn. You will also learn how to optimize classification and regression models through the use of hyperparameter tuning.

#### How good is your model? {.unnumbered}



##### Deciding on a primary metric {.unnumbered}

<div class=""><p>As you have seen, several metrics can be useful to evaluate the performance of classification models, including accuracy, precision, recall, and F1-score.</p>
<p>In this exercise, you will be provided with three different classification problems, and your task is to select the problem where <strong>precision</strong> is best suited as the primary metric.</p></div>

- [ ] A model predicting the presence of cancer as the positive class.
- [ ] A classifier predicting the positive class of a computer program containing malware.
- [x] A model predicting if a customer is a high-value lead for a sales team with limited capacity.

<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct! With limited capacity, the sales team needs the model to return the highest proportion of true positives compared to all predicted positives, thus minimizing wasted effort.</p>

##### Assessing a diabetes prediction classifier {.unnumbered}


<div class>
<p>In this chapter you'll work with the <code>diabetes_df</code> dataset introduced previously. </p>
<p>The goal is to predict whether or not each individual is likely to have diabetes based on the features body mass index (BMI) and age (in years). Therefore, it is a binary classification problem. A target value of <code>0</code> indicates that the individual does <em>not</em> have diabetes, while a value of <code>1</code> indicates that the individual <em>does</em> have diabetes. </p>
<p><code>diabetes_df</code> has been preloaded for you as a pandas DataFrame and split into <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code>. In addition, a <code>KNeighborsClassifier()</code> has been instantiated and assigned to <code>knn</code>.</p>
<p>You will fit the model, make predictions on the test set, then produce a confusion matrix and classification report.</p>
</div>


<li>Import <code>confusion_matrix</code> and <code>classification_report</code>.</li>

<li>Fit the model to the training data.</li>

<li>Predict the labels of the test set, storing the results as <code>y_pred</code>.</li>

<li>Compute and print the confusion matrix and classification report for the test labels versus the predicted labels.</li>
```{python}
# edited/added
df = pd.read_csv('archive/Supervised-Learning-with-scikit-learn/datasets/diabetes_clean.csv')
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Import confusion matrix
from sklearn.metrics import confusion_matrix, classification_report

knn = KNeighborsClassifier(n_neighbors=6)

# Fit the model to the training data
knn.fit(X_train, y_train)

# Predict the labels of the test data: y_pred
y_pred = knn.predict(X_test)

# Generate the confusion matrix and classification report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
```

<p class="">Excellent! The model produced 116 true positives, 34 true negatives, 35 false negatives, and 46 false positives. The classification report shows a better F1-score for the zero class, which represents individuals who do not have diabetes.</p>

#### Logistic regression and the ROC curve {.unnumbered}



##### Building a logistic regression model {.unnumbered}


<div class>
<p>In this exercise, you will build a logistic regression model using all features in the <code>diabetes_df</code> dataset. The model will be used to predict the probability of individuals in the test set having a diabetes diagnosis. </p>
<p>The <code>diabetes_df</code> dataset has been split into <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code>, and preloaded for you.</p>
</div>

<li>Import <code>LogisticRegression</code>.</li>

<li>Instantiate a logistic regression model, <code>logreg</code>.</li>

<li>Fit the model to the training data.</li>

<li>Predict the probabilities of each individual in the test set having a diabetes diagnosis, storing the array of positive probabilities as <code>y_pred_probs</code>.</li>
```{python}
# Import LogisticRegression
from sklearn.linear_model import LogisticRegression

# Instantiate the model
logreg = LogisticRegression()

# Fit the model
logreg.fit(X_train, y_train)

# Predict probabilities
y_pred_probs = logreg.predict_proba(X_test)[:, 1]

print(y_pred_probs[:10])
```

<p class="">Nicely done! Notice how the probability of a diabetes diagnosis for the first 10 individuals in the test set ranges from 0.01 to 0.79. Now let's plot the ROC curve to visualize performance using different thresholds.</p>

##### The ROC curve {.unnumbered}


<div class>
<p>Now you have built a logistic regression model for predicting diabetes status, you can plot the ROC curve to visualize how the true positive rate and false positive rate vary as the decision threshold changes.</p>
<p>The test labels, <code>y_test</code>, and the predicted probabilities of the test features belonging to the positive class, <code>y_pred_probs</code>, have been preloaded for you, along with <code>matplotlib.pyplot</code> as <code>plt</code>.</p>
<p>You will create a ROC curve and then interpret the results.</p>
</div>

<li>Import <code>roc_curve</code>.</li>
```{python}



```
<li>Calculate the ROC curve values, using <code>y_test</code> and <code>y_pred_probs</code>, and unpacking the results into <code>fpr</code>, <code>tpr</code>, and <code>thresholds</code>.</li>

<li>Plot true positive rate against false positive rate.</li>
```{python}
# Import roc_curve
from sklearn.metrics import roc_curve

# Generate ROC curve values: fpr, tpr, thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)

plt.plot([0, 1], [0, 1], 'k--')

# Plot tpr against fpr
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Diabetes Prediction')
plt.show()
```

<div class=""><p>Well done on producing the ROC curve for the diabetes prediction model.</p>
<p>But, what does the plot tell you about the model's performance?</p></div>

- [ ] The model is about as good as randomly guessing the class of each observation.
- [ ] The model is much worse than randomly guessing the class of each observation.
- [x] The model is much better than randomly guessing the class of each observation.
- [ ] It is not possible to conclude whether the model performs better or worse than randomly guessing the class of each observation.

<p class="">Well done! The ROC curve is above the dotted line, so the model performs better than randomly guessing the class of each observation.</p>

##### ROC AUC {.unnumbered}


<div class>
<p>The ROC curve you plotted in the last exercise looked promising.  </p>
<p>Now you will compute the area under the ROC curve, along with the other classification metrics you have used previously.</p>
<p>The <code>confusion_matrix</code> and <code>classification_report</code> functions have been preloaded for you, along with the <code>logreg</code> model you previously built, plus <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, <code>y_test</code>. Also, the model's predicted test set labels are stored as <code>y_pred</code>, and probabilities of test set observations belonging to the positive class stored as <code>y_pred_probs</code>.</p>
<p>A <code>knn</code> model has also been created and the performance metrics printed in the console, so you can compare the <code>roc_auc_score</code>, <code>confusion_matrix</code>, and <code>classification_report</code> between the two models.</p>
</div>

<li>Import <code>roc_auc_score</code>.</li>

<li>Calculate and print the ROC AUC score, passing the test labels and the predicted positive class probabilities.</li>

<li>Calculate and print the confusion matrix.</li>

<li>Call <code>classification_report()</code>.</li>
```{python}
# Import roc_auc_score
from sklearn.metrics import roc_auc_score

# Calculate roc_auc_score
print(roc_auc_score(y_test, y_pred_probs))

# Calculate the confusion matrix
print(confusion_matrix(y_test, y_pred))

# Calculate the classification report
print(classification_report(y_test, y_pred))
```

<p class="">Did you notice that logistic regression performs better than the KNN model across all the metrics you calculated? A ROC AUC score of 0.8002 means this model is 60% better than a chance model at correctly predicting labels! scikit-learn makes it easy to produce several classification metrics with only a few lines of code.</p>

#### Hyperparameter tuning {.unnumbered}



##### Hyperparameter tuning with GridSearchCV {.unnumbered}


<div class>
<p>Now you have seen how to perform grid search hyperparameter tuning, you are going to build a lasso regression model with optimal hyperparameters to predict blood glucose levels using the features in the <code>diabetes_df</code> dataset.</p>
<p><code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code> have  been preloaded for you. A <code>KFold()</code> object has been created and stored for you as <code>kf</code>, along with a lasso regression model as <code>lasso</code>.</p>
</div>

<li>Import <code>GridSearchCV</code>.</li>

<li>Set up a parameter grid for <code>"alpha"</code>, using <code>np.linspace()</code> to create 20 evenly spaced values ranging from <code>0.00001</code> to <code>1</code>.</li>

<li>Call <code>GridSearchCV()</code>, passing <code>lasso</code>, the parameter grid, and setting <code>cv</code> equal to <code>kf</code>.</li>

<li>Fit the grid search object to the training data to perform a cross-validated grid search.</li>
```{python}
# Import GridSearchCV
from sklearn.model_selection import GridSearchCV

# Set up the parameter grid
param_grid = {"alpha": np.linspace(0.00001, 1, 20)}

# Instantiate lasso_cv
lasso_cv = GridSearchCV(lasso, param_grid, cv=kf)

# Fit to the training data
lasso_cv.fit(X_train, y_train)
print("Tuned lasso paramaters: {}".format(lasso_cv.best_params_))
print("Tuned lasso score: {}".format(lasso_cv.best_score_))
```

<p class="">Well done! Unfortunately, the best model only has an R-squared score of <code>0.33</code>, highlighting that using the optimal hyperparameters does not guarantee a high performing model!</p>

##### Hyperparameter tuning with RandomizedSearchCV {.unnumbered}


<div class>
<p>As you saw, <code>GridSearchCV</code> can be computationally expensive, especially if you are searching over a large hyperparameter space. In this case, you can use <code>RandomizedSearchCV</code>, which tests a fixed number of hyperparameter settings from specified probability distributions.</p>
<p>Training and test sets from <code>diabetes_df</code> have been pre-loaded for you as <code>X_train</code>. <code>X_test</code>, <code>y_train</code>, and <code>y_test</code>, where the target is <code>"diabetes"</code>. A logistic regression model has been created and stored as <code>logreg</code>, as well as a <code>KFold</code> variable stored as <code>kf</code>. </p>
<p>You will define a range of hyperparameters and use <code>RandomizedSearchCV</code>, which has been imported from <code>sklearn.model_selection</code>, to look for optimal hyperparameters from these options.</p>
</div>


<li>Create <code>params</code>, adding <code>"l1"</code> and <code>"l2"</code> as <code>penalty</code> values, setting <code>C</code> to a range of <code>50</code> float values between <code>0.1</code> and <code>1.0</code>, and <code>class_weight</code> to either <code>"balanced"</code> or a dictionary containing <code>0:0.8, 1:0.2</code>.</li>

<li>Create the Randomized Search CV object, passing the model and the parameters, and setting <code>cv</code> equal to <code>kf</code>.</li>

<li>Fit <code>logreg_cv</code> to the training data.</li>

<li>Print the model's best parameters and accuracy score.</li>
```{python}
# edited/added
from sklearn.model_selection import RandomizedSearchCV

# Create the parameter space
params = {"penalty": ["l1", "l2"],
         "tol": np.linspace(0.0001, 1.0, 50),
         "C": np.linspace(0.1, 1.0, 50),
         "class_weight": ["balanced", {0:0.8, 1:0.2}]}
         
# Instantiate the RandomizedSearchCV object
logreg_cv = RandomizedSearchCV(logreg, params, cv=kf)

# Fit the data to the model
logreg_cv.fit(X_train, y_train)

# Print the tuned parameters and score
print("Tuned Logistic Regression Parameters: {}".format(logreg_cv.best_params_))
print("Tuned Logistic Regression Best Accuracy Score: {}".format(logreg_cv.best_score_))
```

<p class="">Great searching! Even without exhaustively trying every combination of hyperparameters, the model has an accuracy of over 70% on the test set! So far we have worked with clean datasets; however, in the next chapter, we will discuss the steps required to transform messy data before building supervised learning models.</p>

### Preprocessing and Pipelines {.unnumbered}

Learn how to impute missing values, convert categorical data to numeric values, scale data, evaluate multiple supervised learning models simultaneously, and build pipelines to streamline your workflow!

#### Preprocessing data {.unnumbered}



##### Creating dummy variables {.unnumbered}


<div class>
<p>Being able to include categorical features in the model building process can enhance performance as they may add information that contributes to prediction accuracy.</p>
<p>The <code>music_df</code> dataset has been preloaded for you, and its shape is printed. Also, <code>pandas</code> has been imported as <code>pd</code>. </p>
<p>Now you will create a new DataFrame containing the original columns of <code>music_df</code> plus dummy variables from the <code>"genre"</code> column.</p>
</div>


<li>Use a relevant function, passing the entire <code>music_df</code> DataFrame, to create <code>music_dummies</code>, dropping the first binary column.</li>

<li>Print the shape of <code>music_dummies</code>.</li>
```{python}
# edited/added
music_df = pd.read_csv("archive/Supervised-Learning-with-scikit-learn/datasets/music_clean.csv", index_col=[0])

# Create music_dummies
music_dummies = pd.get_dummies(music_df, drop_first=True)

# Print the new DataFrame's shape
print("Shape of music_dummies: {}".format(music_dummies.shape))
```

<p class="">As there were ten values in the <code>"genre"</code> column, nine new columns were added by a call of <code>pd.get_dummies()</code> using <code>drop_first=True</code>. After dropping the original <code>"genre"</code> column, there are still eight new columns in the DataFrame!</p>

##### Regression with categorical features {.unnumbered}


<div class>
<p>Now you have created <code>music_dummies</code>, containing binary features for each song's genre, it's time to build a ridge regression model to predict song popularity.</p>
<p><code>music_dummies</code> has been preloaded for you, along with <code>Ridge</code>, <code>cross_val_score</code>, <code>numpy</code> as <code>np</code>, and a <code>KFold</code> object stored as <code>kf</code>.</p>
<p>The model will be evaluated by calculating the average RMSE, but first, you will need to convert the scores for each fold to positive values and take their square root. This metric shows the average error of our model's predictions, so it can be compared against the standard deviation of the target value—<code>"popularity"</code>.</p>
</div>

<li>Create <code>X</code>, containing all features in <code>music_dummies</code>, and <code>y</code>, consisting of the <code>"popularity"</code> column, respectively.</li>

<li>Instantiate a ridge regression model, setting <code>alpha</code> equal to 0.2.</li>

<li>Perform cross-validation on <code>X</code> and <code>y</code> using the ridge model, setting <code>cv</code> equal to <code>kf</code>, and using negative mean squared error as the scoring metric.</li>

<li>Print the RMSE values by converting negative <code>scores</code> to positive and taking the square root.</li>
```{python}
# Create X and y
X = music_dummies.drop("popularity", axis=1).values
y = music_dummies["popularity"].values

# Instantiate a ridge model
ridge = Ridge(alpha=0.2)

# Perform cross-validation
scores = cross_val_score(ridge, X, y, cv=kf, scoring="neg_mean_squared_error")

# Calculate RMSE
rmse = np.sqrt(-scores)
print("Average RMSE: {}".format(np.mean(rmse)))
print("Standard Deviation of the target array: {}".format(np.std(y)))
```

<p class="">Great work! An average RMSE of approximately <code>8.24</code> is lower than the standard deviation of the target variable (song popularity), suggesting the model is reasonably accurate.</p>

#### Handling missing data {.unnumbered}



##### Dropping missing data {.unnumbered}


<div class>
<p>Over the next three exercises, you are going to tidy the <code>music_df</code> dataset. You will create a pipeline to impute missing values and build a KNN classifier model, then use it to predict whether a song is of the <code>"Rock"</code> genre.</p>
<p>In this exercise specifically, you will drop missing values accounting for less than 5% of the dataset, and convert the <code>"genre"</code> column into a binary feature.</p>
</div>

<li>Print the number of missing values for each column in the <code>music_df</code> dataset, sorted in ascending order.</li>


<li>Remove values for all columns with 50 or fewer missing values.</li>


<li>Convert <code>music_df["genre"]</code> to values of <code>1</code> if the row contains <code>"Rock"</code>, otherwise change the value to <code>0</code>.</li>
```{python}
# Print missing values for each column
print(music_df.isna().sum().sort_values())

# Remove values where less than 5% are missing
music_df = music_df.dropna(subset=["genre", "popularity", "loudness", "liveness", "tempo"])

# Convert genre to a binary feature
music_df["genre"] = np.where(music_df["genre"] == "Rock", 1, 0)

print(music_df.isna().sum().sort_values())
print("Shape of the `music_df`: {}".format(music_df.shape))
```

<p class="">Well done! The dataset has gone from 1000 observations down to 892, but it is now in the correct format for binary classification and the remaining missing values can be imputed as part of a pipeline.</p>

##### Pipeline for song genre prediction: I {.unnumbered}


<div class>
<p>Now it's time to build a pipeline. It will contain steps to impute missing values using the mean for each feature and build a KNN model for the classification of song genre.</p>
<p>The modified <code>music_df</code> dataset that you created in the previous exercise has been preloaded for you, along with <code>KNeighborsClassifier</code> and <code>train_test_split</code>.</p>
</div>

<li>Import <code>SimpleImputer</code> and <code>Pipeline</code>.</li>

<li>Instantiate an imputer.</li>

<li>Instantiate a KNN classifier with three neighbors.</li>

<li>Create <code>steps</code>, a list of tuples containing the imputer variable you created, called <code>"imputer"</code>, followed by the <code>knn</code> model you created, called <code>"knn"</code>.</li>
```{python}
# Import modules
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

# Instantiate an imputer
imputer = SimpleImputer()

# Instantiate a knn model
knn = KNeighborsClassifier(n_neighbors=3)

# Build steps for the pipeline
steps = [("imputer", imputer), 
         ("knn", knn)]
```

<p class="">Perfect pipeline skills! You are now ready to build and evaluate a song genre classification model.</p>

##### Pipeline for song genre prediction: II {.unnumbered}


<div class>
<p>Having set up the steps of the pipeline in the previous exercise, you will now use it on the <code>music_df</code> dataset to classify the genre of songs. What makes pipelines so incredibly useful is the simple interface that they provide. </p>
<p><code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code> have been preloaded for you, and <code>confusion_matrix</code> has been imported from <code>sklearn.metrics</code>.</p>
</div>



<li>Create a pipeline using the steps you previously defined.</li>

<li>Fit the pipeline to the training data.</li>

<li>Make predictions on the test set.</li>

<li>Calculate and print the confusion matrix.</li>
```{python}
# edited/added
imp_mean = imputer
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)

steps = [("imputer", imp_mean),
        ("knn", knn)]

# Create the pipeline
pipeline = Pipeline(steps)

# Fit the pipeline to the training data
pipeline.fit(X_train, y_train)

# Make predictions on the test set
y_pred = pipeline.predict(X_test)

# Print the confusion matrix
print(confusion_matrix(y_test, y_pred))
```

<p class="">Excellent! See how easy it is to scale our model building workflow using pipelines. In this case, the confusion matrix highlights that the model had 79 true positives and 82 true negatives!</p>

#### Centering and scaling {.unnumbered}



##### Centering and scaling for regression {.unnumbered}


<div class>
<p>Now you have seen the benefits of scaling your data, you will use a pipeline to preprocess the <code>music_df</code> features and build a lasso regression model to predict a song's loudness.</p>
<p><code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code> have been created from the <code>music_df</code> dataset, where the target is <code>"loudness"</code> and the features are all other columns in the dataset. <code>Lasso</code> and <code>Pipeline</code> have also been imported for you.</p>
<p>Note that <code>"genre"</code> has been converted to a binary feature where <code>1</code> indicates a rock song, and <code>0</code> represents other genres.</p>
</div>

<li>Import <code>StandardScaler</code>.</li>

<li>Create the steps for the pipeline object, a <code>StandardScaler</code> object called <code>"scaler"</code>, and a lasso model called <code>"lasso"</code> with <code>alpha</code> set to <code>0.5</code>.</li>

<li>Instantiate a pipeline with steps to scale and build a lasso regression model.</li>

<li>Calculate the R-squared value on the test data.</li>
```{python}
# Import StandardScaler
from sklearn.preprocessing import StandardScaler

# Create pipeline steps
steps = [("scaler", StandardScaler()),
         ("lasso", Lasso(alpha=0.5))]
         
# Instantiate the pipeline
pipeline = Pipeline(steps)
pipeline.fit(X_train, y_train)

# Calculate and print R-squared
print(pipeline.score(X_test, y_test))
```

<p class="">Awesome scaling! The model may have only produced an R-squared of <code>0.619</code>, but without scaling this exact model would have only produced a score of <code>0.35</code>, which proves just how powerful scaling can be!</p>

##### Centering and scaling for classification {.unnumbered}


<div class>
<p>Now you will bring together scaling and model building into a pipeline for cross-validation.</p>
<p>Your task is to build a pipeline to scale features in the <code>music_df</code> dataset and perform grid search cross-validation using a logistic regression model with different values for the hyperparameter <code>C</code>. The target variable here is <code>"genre"</code>, which contains binary values for rock as <code>1</code> and any other genre as <code>0</code>.</p>
<p><code>StandardScaler</code>, <code>LogisticRegression</code>, and <code>GridSearchCV</code> have all been imported for you.</p>
</div>

<li>Build the steps for the pipeline: a <code>StandardScaler()</code> object named <code>"scaler"</code>, and a logistic regression model named <code>"logreg"</code>.</li>

<li>Create the <code>parameters</code>, searching 20 equally spaced float values ranging from <code>0.001</code> to <code>1.0</code> for the logistic regression model's <code>C</code> hyperparameter within the pipeline.</li>

<li>Instantiate the grid search object.</li>

<li>Fit the grid search object to the training data.</li>
```{python}
# Build the steps
steps = [("scaler", StandardScaler()),
         ("logreg", LogisticRegression())]
pipeline = Pipeline(steps)

# Create the parameter space
parameters = {"logreg__C": np.linspace(0.001, 1.0, 20)}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
                                                    random_state=21)
                                                    
# Instantiate the grid search object
cv = GridSearchCV(pipeline, param_grid=parameters)

# Fit to the training data
cv.fit(X_train, y_train)
print(cv.best_score_, "\n", cv.best_params_)
```

<p class="">Well done! Using a pipeline shows that a logistic regression model with <code>"C"</code> set to approximately <code>0.1</code> produces a model with <code>0.8425</code> accuracy!</p>

#### Evaluating multiple models {.unnumbered}



##### Visualizing regression model performance {.unnumbered}


<div class>
<p>Now you have seen how to evaluate multiple models out of the box, you will build three regression models to predict a song's <code>"energy"</code> levels.</p>
<p>The <code>music_df</code> dataset has had dummy variables for <code>"genre"</code> added. Also, feature and target arrays have been created, and these have been split into <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code>. </p>
<p>The following have been imported for you: <code>LinearRegression</code>, <code>Ridge</code>, <code>Lasso</code>, <code>cross_val_score</code>, and <code>KFold</code>.</p>
</div>

<li>Write a for loop using <code>model</code> as the iterator, and <code>model.values()</code> as the iterable.</li>

<li>Perform cross-validation on the training features and the training target array using the model, setting <code>cv</code> equal to the <code>KFold</code> object.</li>

<li>Append the model's cross-validation scores to the results list.</li>

<li>Create a box plot displaying the results, with the x-axis labels as the names of the models.</li>
```{python}
models = {"Linear Regression": LinearRegression(), "Ridge": Ridge(alpha=0.1), "Lasso": Lasso(alpha=0.1)}
results = []

# Loop through the models' values
for model in models.values():
  kf = KFold(n_splits=6, random_state=42, shuffle=True)
  
  # Perform cross-validation
  cv_scores = cross_val_score(model, X_train, y_train, cv=kf)
  
  # Append the results
  results.append(cv_scores)
  
# Create a box plot of the results
plt.boxplot(results, labels=models.keys())
plt.show()
```

<p class="">Nicely done! Lasso regression is not a good model for this problem, while linear regression and ridge perform fairly equally. Let's make predictions on the test set, and see if the RMSE can guide us on model selection.</p>

##### Predicting on the test set {.unnumbered}


<div class>
<p>In the last exercise, linear regression and ridge appeared to produce similar results. It would be appropriate to select either of those models; however, you can check predictive performance on the test set to see if either one can outperform the other.</p>
<p>You will use root mean squared error (RMSE) as the metric. The dictionary <code>models</code>, containing the names and instances of the two models, has been preloaded for you along with the training and target arrays <code>X_train_scaled</code>, <code>X_test_scaled</code>, <code>y_train</code>, and <code>y_test</code>.</p>
</div>


<li>Import <code>mean_squared_error</code>.</li>

<li>Fit the model to the scaled training features and the training labels.</li>

<li>Make predictions using the scaled test features.</li>

<li>Calculate RMSE by passing the test set labels and the predicted labels.</li>
```{python}
# edited/added
from sklearn.preprocessing import scale
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=21)
X_train_scaled = scale(X_train)
X_test_scaled = scale(X_test)

# Import mean_squared_error
from sklearn.metrics import mean_squared_error

for name, model in models.items():
  
  # Fit the model to the training data
  model.fit(X_train_scaled, y_train)
  
  # Make predictions on the test set
  y_pred = model.predict(X_test_scaled)
  
  # Calculate the test_rmse
  test_rmse = mean_squared_error(y_test, y_pred, squared=False)
  print("{} Test Set RMSE: {}".format(name, test_rmse))
```

<p class="">The linear regression model just edges the best performance, although the difference is a RMSE of 0.00001 for popularity! Now let's look at classification model selection.</p>

##### Visualizing classification model performance {.unnumbered}


<div class>
<p>In this exercise, you will be solving a classification problem where the <code>"popularity"</code> column in the <code>music_df</code> dataset has been converted to binary values, with <code>1</code> representing popularity more than or equal to the median for the <code>"popularity"</code> column, and <code>0</code> indicating popularity below the median.</p>
<p>Your task is to build and visualize the results of three different models to classify whether a song is popular or not.</p>
<p>The data has been split, scaled, and preloaded for you as <code>X_train_scaled</code>, <code>X_test_scaled</code>, <code>y_train</code>, and <code>y_test</code>. Additionally, <code>KNeighborsClassifier</code>, <code>DecisionTreeClassifier</code>, and <code>LogisticRegression</code> have been imported.</p>
</div>


<li>Create a dictionary of <code>"Logistic Regression"</code>, <code>"KNN"</code>, and <code>"Decision Tree Classifier"</code>, setting the dictionary's values to a call of each model.</li>

<li>Loop through the values in <code>models</code>.</li>

<li>Instantiate a <code>KFold</code> object to perform 6 splits, setting <code>shuffle</code> to <code>True</code> and <code>random_state</code> to <code>12</code>.</li>

<li>Perform cross-validation using the model, the scaled training features, the target training set, and setting <code>cv</code> equal to <code>kf</code>.</li>
```{python}
# edited/added
from sklearn.tree import DecisionTreeClassifier

# Create models dictionary
models = {"Logistic Regression": LogisticRegression(), "KNN": KNeighborsClassifier(), "Decision Tree Classifier": DecisionTreeClassifier()}
results = []

# Loop through the models' values
for model in models.values():
  
  # Instantiate a KFold object
  kf = KFold(n_splits=6, random_state=12, shuffle=True)
  
  # Perform cross-validation
  cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)
  results.append(cv_results)
plt.boxplot(results, labels=models.keys())
plt.show()
```

<p class="">Looks like logistic regression is the best candidate based on the cross-validation results! Let's wrap up by building a pipeline</p>

##### Pipeline for predicting song popularity {.unnumbered}


<div class>
<p>For the final exercise, you will build a pipeline to impute missing values, scale features, and perform hyperparameter tuning of a logistic regression model. The aim is to find the best parameters and accuracy when predicting song genre! </p>
<p>All the models and objects required to build the pipeline have been preloaded for you.</p>
</div>

<li>Create the steps for the pipeline by calling a simple imputer, a standard scaler, and a logistic regression model.</li>

<li>Create a pipeline object, and pass the <code>steps</code> variable.</li>

<li>Instantiate a grid search object to perform cross-validation using the pipeline and the parameters.</li>

<li>Print the best parameters and compute and print the test set accuracy score for the grid search object.</li>
```{python}
# Create steps
steps = [("imp_mean", SimpleImputer()), 
         ("scaler", StandardScaler()), 
         ("logreg", LogisticRegression())]
         
# Set up pipeline
pipeline = Pipeline(steps)
params = {"logreg__solver": ["newton-cg", "saga", "lbfgs"],
         "logreg__C": np.linspace(0.001, 1.0, 10)}
         
# Create the GridSearchCV object
tuning = GridSearchCV(pipeline, param_grid=params)
tuning.fit(X_train, y_train)
y_pred = tuning.predict(X_test)

# Compute and print performance
print("Tuned Logistic Regression Parameters: {}, Accuracy: {}".format(tuning.best_params_, tuning.score(X_test, y_test)))
```

<p class="">Excellent - you've selected a model, built a preprocessing pipeline, and performed hyperparameter tuning to create a model that is 82% accurate in predicting song genres!</p>

#### Congratulations {.unnumbered}

##### Congratulations {.unnumbered}

Well done on completing the course, I predicted that you would!

##### What you've covered {.unnumbered}

To recap, you have learned the fundamentals of using supervised learning techniques to build predictive models for both regression and classification problems. You have learned the concepts of underfitting and overfitting, how to split data, and perform cross-validation.

##### What you've covered {.unnumbered}

You also learned about data preprocessing techniques, selected which model to build, performed hyperparameter tuning, assessed model performance, and used pipelines!

##### Where to go from here? {.unnumbered}

We covered several models, but there are plenty of others, so to learn more we recommend checking out some of our courses. We also have courses that dive deeper into topics we introduced, such as preprocessing, or model validation. There are other courses on topics we did not cover, such as feature engineering, and unsupervised learning. Additionally, we have many machine learning projects where you can apply the skills you've learned here!

##### Thank you! {.unnumbered}

Congratulations again, and thank you for taking the course! I hope you enjoy using scikit-learn for your supervised learning problems from now on!
