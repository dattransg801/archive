## Machine Learning with Caret {.unnumbered}

<h3 class="course__description-title">Zachary Deane-Mayer</h3>
<p class="course__instructor-description display-none-mobile-course-page-experiment">
    Zach is a Data Scientist at DataRobot and co-author of the caret R package. He's fascinated by predicting the future and spends his free time competing in predictive modeling competitions. He's currently one of top 500 data scientists on Kaggle and took 9th place in the Heritage Health Prize as part of the Analytics Inside team.
  </p>

**Course Description**

<p class="course__description">Machine learning is the study and application of algorithms that learn from and make predictions on data. From search results to self-driving cars, it has manifested itself in all areas of our lives and is one of the most exciting and fast growing fields of research in the world of data science. This course teaches the big ideas in machine learning: how to build and evaluate predictive models, how to tune them for optimal performance, how to preprocess data for better results, and much more. The popular <code>caret</code> R package, which provides a consistent interface to all of R's most powerful machine learning facilities, is used throughout the course.</p>

### Regression {.unnumbered}

<p class="chapter__description">
    In the first chapter of this course, you'll fit regression models with <code>train()</code> and evaluate their out-of-sample performance using cross-validation and root-mean-square error (RMSE).
  </p>
  
#### Welcome to the Toolbox {.unnumbered}



##### In-sample RMSE for linear regression {.unnumbered}

<p>RMSE is commonly calculated in-sample on your training set. What's a potential drawback to calculating training set error?</p>


- [ ] There's no potential drawback to calculating training set error, but you should calculate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="0" style="font-size: 116.7%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container> instead of RMSE.
- [x] You have no idea how well your model generalizes to new data (i.e. overfitting).
- [ ] You should manually inspect your model to validate its coefficients and calculate RMSE.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct! Training set error doesn't tell you anything about the future.</p>

##### In-sample RMSE for linear regression on diamonds {.unnumbered}


<div class>
<p>As you saw in the video, included in the course is the <code>diamonds</code> dataset, which is a classic dataset from the <code>ggplot2</code> package. The dataset contains physical attributes of diamonds as well as the price they sold for. One interesting modeling challenge is predicting diamond price based on their attributes using something like a linear regression.</p>
<p>Recall that to fit a linear regression, you use the <code>lm()</code> function in the following format:</p>
<pre><code>mod &lt;- lm(y ~ x, my_data)
</code></pre>
<p>To make predictions using <code>mod</code> on the original data, you call the <code>predict()</code> function:</p>
<pre><code>pred &lt;- predict(mod, my_data)
</code></pre>
</div>

<li>Fit a linear model on the <code>diamonds</code> dataset predicting <code>price</code> using all other variables as predictors (i.e. <code>price ~ .</code>). Save the result to <code>model</code>.</li>


<li>Make predictions using <code>model</code> on the full original dataset and save the result to <code>p</code>.</li>


<li>Compute errors using the formula \(errors = predicted - actual\). Save the result to <code>error</code>.</li>


<li>Compute RMSE using the formula you learned in the video and print it to the console.</li>
```{r}
# edited/added
library(tidyverse)
data("diamonds")

# Fit lm model: model
model <- lm(price ~ ., diamonds)

# Predict on full data: p
p <- predict(model, diamonds)

# Compute errors: error
error <- p - diamonds[["price"]]

# Calculate RMSE
sqrt(mean(error ^ 2))
```

<p class="">Great work! Now you know how to manually calculate RMSE for your model's predictions!
</p>


#### Out-of-sample error measures {.unnumbered}



##### Out-of-sample RMSE for linear regression {.unnumbered}

<p>What is the advantage of using a train/test split rather than just validating your model in-sample on the training set?</p>


- [ ] It takes less time to calculate error on the test set, since it is smaller than the training set.
- [ ] There is no advantage to using a test set. You can just use adjusted <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="2" style="font-size: 117.4%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container> on your training set.
- [x] It gives you an estimate of how well your model performs on new data.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct!  Tests sets are essential for making sure your models will make good predictions.</p>

##### Randomly order the data frame {.unnumbered}


<div class>
<p>One way you can take a train/test split of a dataset is to order the dataset randomly, then divide it into the two sets. This ensures that the training set and test set are both random samples and that any biases in the ordering of the dataset (e.g. if it had originally been ordered by price or size) are not retained in the samples we take for training and testing your models. You can think of this like shuffling a brand new deck of playing cards before dealing hands.</p>
<p>First, you set a random seed so that your work is reproducible and you get the same random split each time you run your script:</p>
<pre><code>set.seed(42)
</code></pre>
<p>Next, you use the <code>sample()</code> function to shuffle the row indices of the <code>diamonds</code> dataset. You can later use these indices to reorder the dataset.</p>
<pre><code>rows &lt;- sample(nrow(diamonds))
</code></pre>
<p>Finally, you can use this random vector to reorder the diamonds dataset:</p>
<pre><code>diamonds &lt;- diamonds[rows, ]
</code></pre>
</div>

<li>Set the random seed to 42.</li>


<li>Make a vector of row indices called <code>rows</code>.</li>


<li>Randomly reorder the <code>diamonds</code> data frame, assigning to <code>shuffled_diamonds</code>.</li>
```{r}
# Set seed
set.seed(42)

# Shuffle row indices: rows
rows <- sample(nrow(diamonds))

# Randomly order data
shuffled_diamonds <- diamonds[rows, ]
```

<p class="">Great job! Randomly ordering your dataset is important for many machine learning methods.
</p>


##### Try an 80/20 split {.unnumbered}


<div class>
<p>Now that your dataset is randomly ordered, you can split the first 80% of it into a training set, and the last 20% into a test set. You can do this by choosing a split point approximately 80% of the way through your data:</p>
<pre><code>split &lt;- round(nrow(mydata) * 0.80)
</code></pre>
<p>You can then use this point to break off the first 80% of the dataset as a training set:</p>
<pre><code>mydata[1:split, ]
</code></pre>
<p>And then you can use that same point to determine the test set:</p>
<pre><code>mydata[(split + 1):nrow(mydata), ]
</code></pre>
</div>

<li>Choose a row index to split on so that the split point is approximately 80% of the way through the <code>diamonds</code> dataset. Call this index <code>split</code>.</li>


<li>Create a training set called <code>train</code> using that index.</li>


<li>Create a test set called <code>test</code> using that index.</li>
```{r}
# Determine row to split on: split
split <- round(nrow(diamonds) * 0.80)

# Create train
train <- diamonds[1:split, ]

# Create test
test <- diamonds[(split + 1):nrow(diamonds), ]
```

<p class="">Well done! Because you already randomly ordered your dataset, it's easy to split off a random test set.
</p>


##### Predict on test set {.unnumbered}


<div class>
<p>Now that you have a randomly split training set and test set, you can use the <code>lm()</code> function as you did in the first exercise to fit a model to your training set, rather than the entire dataset.  Recall that you can use the formula interface to the linear regression function to fit a model with a specified target variable using all other variables in the dataset as predictors:</p>
<pre><code>mod &lt;- lm(y ~ ., training_data)
</code></pre>
<p>You can use the <code>predict()</code> function to make predictions from that model on new data.  The new dataset must have all of the columns from the training data, but they can be in a different order with different values. Here, rather than re-predicting on the training set, you can predict on the test set, which you did not use for training the model. This will allow you to determine the out-of-sample error for the model in the next exercise:</p>
<pre><code>p &lt;- predict(model, new_data)
</code></pre>
</div>

<li>Fit an <code>lm()</code> model called <code>model</code> to predict <code>price</code> using all other variables as covariates. Be sure to use the training set, <code>train</code>.</li>


<li>Predict on the test set, <code>test</code>, using <code>predict()</code>. Store these values in a vector called <code>p</code>.</li>
```{r}
# Fit lm model on train: model
model <- lm(price ~ ., train)

# Predict on test: p
p <- predict(model, test)
```

<p class="">Excellent work! R makes it very easy to predict with a model on new data.
</p>


##### Calculate test set RMSE by hand {.unnumbered}


<div class>
<p>Now that you have predictions on the test set, you can use these predictions to calculate an error metric (in this case RMSE) on the test set and see how the model performs out-of-sample, rather than in-sample as you did in the first exercise.  You first do this by calculating the errors between the predicted diamond prices and the actual diamond prices by subtracting the predictions from the actual values.</p>
<p>Once you have an error vector, calculating RMSE is as simple as squaring it, taking the mean, then taking the square root:</p>
<pre><code>sqrt(mean(error^2))
</code></pre>
</div>
<div class="exercise--instructions__content">
<p><code>test</code>, <code>model</code>, and <code>p</code> are loaded in your workspace.</p>

<li>Calculate the error between the predictions on the test set and the actual diamond prices in the test set. Call this <code>error</code>.</li>


<li>Calculate RMSE using this error vector, just printing the result to the console.</li>
```{r}
# Compute errors: error
error <- p - test[["price"]]

# Calculate RMSE
sqrt(mean(error^2))
```


</div>

<p class="">Good Job! Calculating RMSE on a test set is exactly the same as calculating it on a training set.
</p>

##### Comparing out-of-sample RMSE to in-sample RMSE {.unnumbered}

<p>Why is the test set RMSE <em>higher</em> than the training set RMSE?</p>


- [x] Because you overfit the training set and the test set contains data the model hasn't seen before.
- [ ] Because you should not use a test set at all and instead just look at error on the training set.
- [ ] Because the test set has a smaller sample size the training set and thus the mean error is lower.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Right! Computing the error on the training set is risky because the model may overfit the data used to train it.</p>

#### Cross-validation {.unnumbered}



##### Advantage of cross-validation {.unnumbered}

<p>What is the advantage of cross-validation over a single train/test split?</p>


- [ ] There is no advantage to cross-validation, just as there is no advantage to a single train/test split.  You should be validating your models in-sample with a metric like adjusted <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="3" style="font-size: 116.7%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container>.
- [ ] You can pick the best test set to minimize the reported RMSE of your model.
- [x] It gives you multiple estimates of out-of-sample error, rather than a single estimate.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct! If all of your estimates give similar outputs, you can be more certain of the model's accuracy. If your estimates give different outputs, that tells you the model does not perform consistently and suggests a problem with it.</p>

##### 10-fold cross-validation {.unnumbered}


<div class>
<p>As you saw in the video, a better approach to validating models is to use multiple systematic test sets, rather than a single random train/test split. Fortunately, the <code>caret</code> package makes this very easy to do:</p>
<pre><code>model &lt;- train(y ~ ., my_data)
</code></pre>
<p><code>caret</code> supports many types of cross-validation, and you can specify which type of cross-validation and the number of cross-validation folds with the <code>trainControl()</code> function, which you pass to the <code>trControl</code> argument in <code>train()</code>:</p>
<pre><code>model &lt;- train(
  y ~ ., 
  my_data,
  method = "lm",
  trControl = trainControl(
    method = "cv", 
    number = 10,
    verboseIter = TRUE
  )
)
</code></pre>
<p>It's important to note that you pass the method for modeling to the main <code>train()</code> function and the method for cross-validation to the <code>trainControl()</code> function.</p>
</div>

<li>Fit a linear regression to model <code>price</code> using all other variables in the <code>diamonds</code> dataset as predictors. Use the <code>train()</code> function and 10-fold cross-validation. (Note that we've taken a subset of the full <code>diamonds</code> dataset to speed up this operation, but it's still named <code>diamonds</code>.)</li>


<li>Print the model to the console and examine the results.</li>
```{r}
# edited/added
library(caret)

# Fit lm model using 10-fold CV: model
model <- train(
  price ~ ., 
  diamonds,
  method = "lm",
  trControl = trainControl(
    method = "cv", 
    number = 10,
    verboseIter = TRUE
  )
)

# Print model to console
model
```

<p class="">Good job! Caret does all the work of splitting test sets and calculating RMSE for you!
</p>


##### 5-fold cross-validation {.unnumbered}


<div class>
<p>In this course, you will use a wide variety of datasets to explore the full flexibility of the <code>caret</code> package. Here, you will use the famous Boston housing dataset, where the goal is to predict median home values in various Boston suburbs.</p>
<p>You can use exactly the same code as in the previous exercise, but change the dataset used by the model:</p>
<pre><code>model &lt;- train(
  medv ~ ., 
  Boston, # &lt;- new!
  method = "lm",
  trControl = trainControl(
    method = "cv", 
    number = 10,
    verboseIter = TRUE
  )
)
</code></pre>
<p>Next, you can reduce the number of cross-validation folds from 10 to 5 using the <code>number</code> argument to the <code>trainControl()</code> argument:</p>
<pre><code>trControl = trainControl(
  method = "cv", 
  number = 5,
  verboseIter = TRUE
)
</code></pre>
</div>

<li>Fit an <code>lm()</code> model to the <code>Boston</code> housing dataset, such that <code>medv</code> is the response variable and all other variables are explanatory variables.</li>


<li>Use 5-fold cross-validation rather than 10-fold cross-validation.</li>


<li>Print the model to the console and inspect the results.</li>
```{r}
# edited/added
library(mlbench)
data(BostonHousing)
Boston = BostonHousing

# Fit lm model using 5-fold CV: model
model <- train(
  medv ~ ., 
  Boston,
  method = "lm",
  trControl = trainControl(
    method = "cv", 
    number = 5,
    verboseIter = TRUE
  )
)

# Print model to console
model
```

<p class="">Great work! Caret makes it easy to try different validation schemes with the same model and compare RMSE.
</p>


##### 5 x 5-fold cross-validation {.unnumbered}


<div class>
<p>You can do more than just one iteration of cross-validation. Repeated cross-validation gives you a better estimate of the test-set error. You can also repeat the entire cross-validation procedure. This takes longer, but gives you many more out-of-sample datasets to look at and much more precise assessments of how well the model performs.</p>
<p>One of the awesome things about the <code>train()</code> function in <code>caret</code> is how easy it is to run very different models or methods of cross-validation just by tweaking a few simple arguments to the function call. For example, you could repeat your entire cross-validation procedure 5 times for greater confidence in your estimates of the model's out-of-sample accuracy, e.g.:</p>
<pre><code>trControl = trainControl(
  method = "repeatedcv", 
  number = 5,
  repeats = 5, 
  verboseIter = TRUE
)
</code></pre>
</div>

<li>Re-fit the linear regression model to the <code>Boston</code> housing dataset.</li>


<li>Use 5 repeats of 5-fold cross-validation.</li>


<li>Print the model to the console.</li>
```{r}
# Fit lm model using 5 x 5-fold CV: model
model <- train(
  medv ~ ., 
  Boston,
  method = "lm",
  trControl = trainControl(
    method = "repeatedcv", 
    number = 5,
    repeats = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model
```

<p class="">Fantastic work! You can use caret to do some very complicated cross-validation schemes.
</p>


##### Making predictions on new data {.unnumbered}


<div class>
<p>Finally, the model you fit with the <code>train()</code> function has the exact same <code>predict()</code> interface as the linear regression models you fit earlier in this chapter.</p>
<p>After fitting a model with <code>train()</code>, you can simply call <code>predict()</code> with new data, e.g:</p>
<pre><code>predict(my_model, new_data)
</code></pre>
</div>
<div class="exercise--instructions__content"><p>Use the <code>predict()</code> function to make predictions with <code>model</code> on the full <code>Boston</code> housing dataset. Print the result to the console.</p></div>
```{r}
# Predict on full Boston dataset
predict(model, Boston)
```

<p class="">Awesome job! Predicting with a caret model is as easy as predicting with a regular model!
</p>

### Classification {.unnumbered}

<p class="chapter__description">
    In this chapter, you'll fit classification models with <code>train()</code> and evaluate their out-of-sample performance using cross-validation and area under the curve (AUC).
  </p>
  
#### Logistic regression on sonar {.unnumbered}



##### Why a train/test split? {.unnumbered}

<p>What is the point of making a train/test split for binary classification problems?</p>


- [ ] To make the problem harder for the model by reducing the dataset size.
- [x] To evaluate your models out-of-sample, on new data.
- [ ] To reduce the dataset size, so your models fit faster.
- [ ] There is no real reason; it is no different than evaluating your models in-sample.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct! Out-of-sample evaluation is the gold standard of model validation.</p>

##### Try a 60/40 split {.unnumbered}


<div class>
<p>As you saw in the video, you'll be working with the <code>Sonar</code> dataset in this chapter, using a 60% training set and a 40% test set.  We'll practice making a train/test split one more time, just to be sure you have the hang of it. Recall that you can use the <code>sample()</code> function to get a random permutation of the row indices in a dataset, to use when making train/test splits, e.g.:</p>
<pre><code>n_obs &lt;- nrow(my_data)
permuted_rows &lt;- sample(n_obs)
</code></pre>
<p>And then use those row indices to randomly reorder the dataset, e.g.:</p>
<pre><code>my_data &lt;- my_data[permuted_rows, ]
</code></pre>
<p>Once your dataset is randomly ordered, you can split off the first 60% as a training set and the last 40% as a test set.</p>
</div>

<li>Get the number of observations (rows) in <code>Sonar</code>, assigning to <code>n_obs</code>.</li>


<li>Shuffle the row indices of <code>Sonar</code> and store the result in <code>permuted_rows</code>.</li>


<li>Use <code>permuted_rows</code> to randomly reorder the rows of <code>Sonar</code>, saving as <code>Sonar_shuffled</code>.</li>


<li>Identify the proper row to split on for a 60/40 split. Store this row number as <code>split</code>.</li>


<li>Save the first 60% of <code>Sonar_shuffled</code> as a training set.</li>


<li>Save the last 40% of <code>Sonar_shuffled</code> as the test set.</li>
```{r}
# edited/added
data(Sonar)

# Get the number of observations
n_obs <- nrow(Sonar)

# Shuffle row indices: permuted_rows
permuted_rows <- sample(n_obs)

# Randomly order data: Sonar
Sonar_shuffled <- Sonar[permuted_rows, ]

# Identify row to split on: split
split <- round(n_obs * 0.6)

# Create train
train <- Sonar_shuffled[1:split, ]

# Create test
test <- Sonar_shuffled[(split + 1):n_obs, ]
```

<p class="">Excellent work! Randomly shuffling your data makes it easy to manually create a train/test split.
</p>


##### Fit a logistic regression model {.unnumbered}


<div class>
<p>Once you have your random training and test sets you can fit a logistic regression model to your training set using the <code>glm()</code> function. <code>glm()</code> is a more advanced version of <code>lm()</code> that allows for more varied types of regression models, aside from plain vanilla ordinary least squares regression.</p>
<p>Be sure to pass the argument <code>family = "binomial"</code> to <code>glm()</code> to specify that you want to do logistic (rather than linear) regression. For example:</p>
<pre><code>glm(Target ~ ., family = "binomial", dataset)
</code></pre>
<p>Don't worry about warnings like <code>glm.fit: algorithm did not converge</code> or <code>glm.fit: fitted probabilities numerically 0 or 1 occurred</code>.  These are common on smaller datasets and usually don't cause any issues.  They typically mean your dataset is <em>perfectly separable</em>, which can cause problems for the math behind the model, but R's <code>glm()</code> function is almost always robust enough to handle this case with no problems.</p>
<p>Once you have a <code>glm()</code> model fit to your dataset, you can predict the outcome (e.g. rock or mine) on the <code>test</code> set using the <code>predict()</code> function with the argument <code>type = "response"</code>:</p>
<pre><code>predict(my_model, test, type = "response")
</code></pre>
</div>

<li>Fit a logistic regression called <code>model</code> to predict <code>Class</code> using all other variables as predictors. Use the training set for <code>Sonar</code>.</li>


<li>Predict on the <code>test</code> set using that model. Call the result <code>p</code> like you've done before.</li>
```{r}
# Fit glm model: model
model <- glm(Class ~ ., family = "binomial", train)

# Predict on test: p
p <- predict(model, test, type = "response")
```

<p class="">Great work! Manually fitting a glm model in R is very similar to fitting an lm model.
</p>


#### Confusion matrix {.unnumbered}



##### Confusion matrix takeaways {.unnumbered}

<p>What information does a confusion matrix provide?</p>


- [ ] True positive rates
- [ ] True negative rates
- [ ] False positive rates
- [ ] False negative rates
- [x] All of the above


<p class="dc-completion-pane__message dc-u-maxw-100pc">Yes! It contains all of them.</p>

##### Calculate a confusion matrix {.unnumbered}


<div class>
<p>As you saw in the video, a confusion matrix is a very useful tool for calibrating the output of a model and examining all possible outcomes of your predictions (true positive, true negative, false positive, false negative).</p>
<p>Before you make your confusion matrix, you need to "cut" your predicted probabilities at a given threshold to turn probabilities into a factor of class predictions. Combine <code>ifelse()</code> with <code>factor()</code> as follows:</p>
<pre><code>pos_or_neg &lt;- ifelse(probability_prediction &gt; threshold, positive_class, negative_class)
p_class &lt;- factor(pos_or_neg, levels = levels(test_values))
</code></pre>
<p><code>confusionMatrix()</code> in <code>caret</code> improves on <code>table()</code> from base R by adding lots of useful ancillary statistics in addition to the base rates in the table. You can calculate the confusion matrix (and the associated statistics) using the predicted outcomes as well as the actual outcomes, e.g.:</p>
<pre><code>confusionMatrix(p_class, test_values)
</code></pre>
</div>

<li>Use <code>ifelse()</code> to create a character vector, <code>m_or_r</code> that is the positive class, <code>"M"</code>, when <code>p</code> is greater than 0.5, and the negative class, <code>"R"</code>, otherwise.</li>


<li>Convert <code>m_or_r</code> to be a factor, <code>p_class</code>, with levels the same as those of <code>test[["Class"]]</code>.</li>


<li>Make a confusion matrix with <code>confusionMatrix()</code>, passing <code>p_class</code> and the <code>"Class"</code> column from the <code>test</code> dataset.</li>
```{r}
# If p exceeds threshold of 0.5, M else R: m_or_r
m_or_r <- ifelse(p > 0.5, "M", "R")

# Convert to factor: p_class
p_class <- factor(m_or_r, levels = levels(test[["Class"]]))

# Create confusion matrix
confusionMatrix(p_class, test[["Class"]])
```

<p class="">Great work! The confusionMatrix function is a very easy way to get a detailed summary of your model's accuracy.
</p>


##### Calculating accuracy {.unnumbered}


<div class>
<p>Use <code>confusionMatrix(p_class, test[["Class"]])</code> to calculate a confusion matrix on the test set.</p>
<p>What is the test set accuracy of this model (rounded to the nearest percent)?</p>
</div>


- [ ] 58%
- [ ] 83%
- [x] 70%
- [ ] 51%


<p class="">Nice one! This is the model's accuracy.
</p>

##### Calculating true positive rate {.unnumbered}


<div class>
<p>Use <code>confusionMatrix(p_class, test[["Class"]])</code> to calculate a confusion matrix on the test set.</p>
<p>What is the test set true positive rate (or sensitivity) of this model (rounded to the nearest percent)?</p>
</div>


- [ ] 58%
- [x] 83%
- [ ] 70%
- [ ] 51%


<p class="">Nice one!
</p>

##### Calculating true negative rate {.unnumbered}


<div class>
<p>Use <code>confusionMatrix(p_class, test[["Class"]])</code> to calculate a confusion matrix on the test set.</p>
<p>What is the test set true negative rate (or specificity) of this model (rounded to the nearest percent)?</p>
</div>


- [ ] 58%
- [ ] 83%
- [ ] 70%
- [x] 51%


<p class="">Good job!
</p>

#### Class probabilities and predictions {.unnumbered}



##### Probabilities and classes {.unnumbered}

<p>What's the relationship between the predicted probabilities and the predicted classes?</p>


- [ ] You determine the predicted probabilities by looking at the average accuracy of the predicted classes.
- [ ] There is no relationship; they're completely different things.
- [x] Predicted classes are based off of predicted probabilities plus a classification threshold.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct! Probabilities are used to determine classes.</p>

##### Try another threshold {.unnumbered}


<div class>
<p>In the previous exercises, you used a threshold of 0.50 to cut your predicted probabilities to make class predictions (rock vs mine).  However, this classification threshold does not always align with the goals for a given modeling problem.</p>
<p>For example, pretend you want to identify the objects you are really certain are mines.  In this case, you might want to use a probability threshold of 0.90 to get <em>fewer predicted mines, but with greater confidence in each prediction</em>.</p>
<p>The code pattern for cutting probabilities into predicted classes, then calculating a confusion matrix, was shown in Exercise 7 of this chapter.</p>
</div>

<li>Use <code>ifelse()</code> to create a character vector, <code>m_or_r</code> that is the positive class, <code>"M"</code>, when <code>p</code> is greater than <strong>0.9</strong>, and the negative class, <code>"R"</code>, otherwise.</li>


<li>Convert <code>m_or_r</code> to be a factor, <code>p_class</code>, with levels the same as those of <code>test[["Class"]]</code>.</li>


<li>Make a confusion matrix with <code>confusionMatrix()</code>, passing <code>p_class</code> and the <code>"Class"</code> column from the <code>test</code> dataset.</li>
```{r}
# If p exceeds threshold of 0.9, M else R: m_or_r
m_or_r <- ifelse(p > 0.9, "M", "R")

# Convert to factor: p_class
p_class <- factor(m_or_r, levels = levels(test[["Class"]]))

# Create confusion matrix
confusionMatrix(p_class, test[["Class"]])
```

<p class="">Amazing! Note that there are (slightly) fewer predicted mines with this higher threshold: 55 (40 + 15) as compared to 57 for the 0.50 threshold.
</p>


##### From probabilites to confusion matrix {.unnumbered}


<div class>
<p>Conversely, say you want to be really certain that your model correctly identifies all the mines as mines. In this case, you might use a prediction threshold of 0.10, instead of 0.90.</p>
<p>The code pattern for cutting probabilities into predicted classes, then calculating a confusion matrix, was shown in Exercise 7 of this chapter.</p>
</div>

<li>Use <code>ifelse()</code> to create a character vector, <code>m_or_r</code> that is the positive class, <code>"M"</code>, when <code>p</code> is greater than <strong>0.1</strong>, and the negative class, <code>"R"</code>, otherwise.</li>


<li>Convert <code>m_or_r</code> to be a factor, <code>p_class</code>, with levels the same as those of <code>test[["Class"]]</code>.</li>


<li>Make a confusion matrix with <code>confusionMatrix()</code>, passing <code>p_class</code> and the <code>"Class"</code> column from the <code>test</code> dataset.</li>
```{r}
# If p exceeds threshold of 0.1, M else R: m_or_r
m_or_r <- ifelse(p > 0.1, "M", "R")

# Convert to factor: p_class
p_class <- factor(m_or_r, levels = levels(test[["Class"]]))

# Create confusion matrix
confusionMatrix(p_class, test[["Class"]])
```

<p class="">Awesome! Note that there are (slightly) more predicted mines with this lower threshold: 58 (40 + 18) as compared to 47 for the 0.50 threshold.
</p>


#### Introducing the ROC curve {.unnumbered}



##### What's the value of a ROC curve? {.unnumbered}

<p>What is the primary value of an ROC curve?</p>


- [ ] It has a cool acronym.
- [ ] It can be used to determine the true positive and false positive rates for a particular classification threshold.
- [x] It evaluates all possible thresholds for splitting predicted probabilities into predicted classes.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Yes! ROC curves let you evaluate how good a model is, without worry about calibrating its probabilities.</p>

##### Plot an ROC curve {.unnumbered}


<div class>
<p>As you saw in the video, an ROC curve is a really useful shortcut for summarizing the performance of a classifier over all possible thresholds. This saves you a lot of tedious work computing class predictions for many different thresholds and examining the confusion matrix for each.</p>
<p>My favorite package for computing ROC curves is <code>caTools</code>, which contains a function called <code>colAUC()</code>.  This function is very user-friendly and can actually calculate ROC curves for multiple predictors at once.  In this case, you only need to calculate the ROC curve for one predictor, e.g.:</p>
<pre><code>colAUC(predicted_probabilities, actual, plotROC = TRUE)
</code></pre>
<p>The function will return a score called AUC (more on that later) and the <code>plotROC = TRUE</code> argument will return the plot of the ROC curve for visual inspection.</p>
</div>
<div class="exercise--instructions__content">
<p><code>model</code>, <code>test</code>, and <code>train</code> from the last exercise using the sonar data are loaded in your workspace.</p>

<li>Predict probabilities (i.e. <code>type = "response"</code>) on the test set, then store the result as <code>p</code>.</li>


<li>Make an ROC curve using the predicted test set probabilities.</li>
```{r}
# edited/added
library(caTools)

# Predict on test: p
p <- predict(model, test, type = "response")

# Make ROC curve
colAUC(p, test[["Class"]], plotROC = TRUE)
```


</div>

<p class="">Great work! The colAUC function makes plotting a roc curve as easy as calculating a confusion matrix.
</p>

#### Area under the curve (AUC) {.unnumbered}



##### Model, ROC, and AUC {.unnumbered}

<p>What is the AUC of a perfect model?</p>


- [ ] 0.00
- [ ] 0.50
- [x] 1.00


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct! A perfect model has an AUC of 1.</p>

##### Customizing trainControl {.unnumbered}


<div class>
<p>As you saw in the video, area under the ROC curve is a very useful, single-number summary of a model's ability to discriminate the positive from the negative class (e.g. mines from rocks).  An AUC of 0.5 is no better than random guessing, an AUC of 1.0 is a perfectly predictive model, and an AUC of 0.0 is perfectly anti-predictive (which rarely happens).</p>
<p>This is often a much more useful metric than simply ranking models by their accuracy at a set threshold, as different models might require different calibration steps (looking at a confusion matrix at each step) to find the optimal classification threshold for that model.</p>
<p>You can use the <code>trainControl()</code> function in <code>caret</code> to use AUC (instead of acccuracy), to tune the parameters of your models. The <code>twoClassSummary()</code> convenience function allows you to do this easily.</p>
<p>When using <code>twoClassSummary()</code>, be sure to always include the argument <code>classProbs = TRUE</code> or your model will throw an error! (You cannot calculate AUC with just class predictions. You need to have class probabilities as well.)</p>
</div>

<li>Customize the <code>trainControl</code> object to use <code>twoClassSummary</code> rather than <code>defaultSummary</code>.</li>


<li>Use 10-fold cross-validation.</li>


<li>Be sure to tell <code>trainControl()</code> to return class probabilities.</li>
```{r}
# Create trainControl object: myControl
myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)
```

<p class="">Great work! Don't forget the classProbs argument to train control, especially if you're going to calculate AUC or logloss.
</p>


##### Using custom trainControl {.unnumbered}


<div class>
<p>Now that you have a custom <code>trainControl</code> object, it's easy to fit <code>caret</code> models that use AUC rather than accuracy to tune and evaluate the model. You can just pass your custom <code>trainControl</code> object to the <code>train()</code> function via the <code>trControl</code> argument, e.g.:</p>
<pre><code>train(&lt;standard arguments here&gt;, trControl = myControl)
</code></pre>
<p>This syntax gives you a convenient way to store a lot of custom modeling parameters and then use them across multiple different calls to <code>train()</code>. You will make extensive use of this trick in Chapter 5.</p>
</div>

<li>Use <code>train()</code> to predict <code>Class</code> from all other variables in the <code>Sonar</code> data (that is, <code>Class ~ .</code>). It should be a <code>glm</code> model (that is, set <code>method</code> to <code>"glm"</code>) using your custom <code>trainControl</code> object, <code>myControl</code>. Save the result to <code>model</code>.</li>


<li>Print the model to the console and examine its output.</li>
```{r}
# Train glm with custom trainControl: model
model <- train(
  Class ~ ., 
  Sonar, 
  method = "glm",
  trControl = myControl
)

# Print model to console
model
```

<p class="">Great work! Note that fitting a glm with caret often produces warnings about convergence or probabilities. These warnings can almost always be safely ignored, as you can use the glm's predictions to validate whether the model is accurate enough for your task.
</p>

### Tuning models {.unnumbered}

<p class="chapter__description">
    In this chapter, you will use the <code>train()</code> function to tweak model parameters through cross-validation and grid search.
  </p>
  
#### Random forests and wine {.unnumbered}



##### Random forests vs. linear models {.unnumbered}

<p>What's the primary advantage of random forests over linear models?</p>


- [ ] They make you sound cooler during job interviews.
- [ ] You can't understand what's going on inside of a random forest model, so you don't have to explain it to anyone.
- [x] A random forest is a more flexible model than a linear model, but just as easy to fit.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct! Random forests are very powerful non-linear models, but are also very easy to fit.</p>

##### Fit a random forest {.unnumbered}


<div class>
<p>As you saw in the video, random forest models are much more flexible than linear models, and can model complicated nonlinear effects as well as automatically capture interactions between variables.  They tend to give very good results on real world data, so let's try one out on the wine quality dataset, where the goal is to predict the human-evaluated quality of a batch of wine, given some of the machine-measured chemical and physical properties of that batch.</p>
<p>Fitting a random forest model is exactly the same as fitting a generalized linear regression model, as you did in the previous chapter. You simply change the <code>method</code> argument in the <code>train</code> function to be <code>"ranger"</code>. The <code>ranger</code> package is a rewrite of R's classic <code>randomForest</code> package and fits models much faster, but gives almost exactly the same results.  We suggest that all beginners use the <code>ranger</code> package for random forest modeling.</p>
</div>

<li>Train a random forest called <code>model</code> on the wine quality dataset, <code>wine</code>, such that <code>quality</code> is the response variable and all other variables are explanatory variables.</li>


<li>Use <code>method = "ranger"</code>.</li>


<li>Use a <code>tuneLength</code> of 1.</li>


<li>Use 5 CV folds.</li>


<li>Print <code>model</code> to the console.</li>
```{r}
# edited/added
library(ranger)
wine=readRDS("archive/Machine-Learning-with-caret-in-R/datasets/wine_100.RDS")

# Fit random forest: model
model <- train(
  quality ~ .,
  tuneLength = 1,
  data = wine, 
  method = "ranger",
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model
```

<p class="">Awesome job! Fitting a random forest is just as easy as fitting a glm.  Caret makes it very easy to try out many different models.
</p>


#### Explore a wider model space {.unnumbered}



##### Advantage of a longer tune length {.unnumbered}

<p>What's the advantage of a longer <code>tuneLength</code>?</p>


- [x] You explore more potential models and can potentially find a better model.
- [ ] Your models take less time to fit.
- [ ] There's no advantage; you'll always end up with the same final model.


<p class="dc-completion-pane__message dc-u-maxw-100pc">You're correct! Longer tune lengths explore more models.</p>

##### Try a longer tune length {.unnumbered}


<div class>
<p>Recall from the video that random forest models have a primary tuning parameter of <code>mtry</code>, which controls how many variables are exposed to the splitting search routine at each split. For example, suppose that a tree has a total of 10 splits and <code>mtry = 2</code>. This means that there are 10 samples of 2 predictors each time a split is evaluated.</p>
<p>Use a larger tuning grid this time, but stick to the defaults provided by the <code>train()</code> function. Try a <code>tuneLength</code> of 3, rather than 1, to explore some more potential models, and plot the resulting model using the <code>plot</code> function.</p>
</div>

<li>Train a random forest model, <code>model</code>, using the <code>wine</code> dataset on the <code>quality</code> variable with all other variables as explanatory variables. (This will take a few seconds to run, so be patient!)</li>


<li>Use <code>method = "ranger"</code>.</li>


<li>Change the <code>tuneLength</code> to 3.</li>


<li>Use 5 CV folds.</li>


<li>Print <code>model</code> to the console.</li>


<li>Plot the model after fitting it.</li>
```{r}
# Fit random forest: model
model <- train(
  quality ~ .,
  tuneLength = 3,
  data = wine, 
  method = "ranger",
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

# Plot model
plot(model)
```

<p class="">Excellent! You can adjust the tuneLength variable to make a trade-off between runtime and how deep you want to grid-search the model.
</p>


#### Custom tuning grids {.unnumbered}



##### Advantages of a custom tuning grid {.unnumbered}

<p>Why use a custom <code>tuneGrid</code>?</p>


- [ ] There's no advantage; you'll always end up with the same final model.
- [x] It gives you more fine-grained control over the tuning parameters that are explored.
- [ ] It always makes your models run faster.


<p class="dc-completion-pane__message dc-u-maxw-100pc">You're right! A custom tune grid gives you full control over caret's grid search.</p>

##### Fit a random forest with custom tuning {.unnumbered}


<div class>
<p>Now that you've explored the default tuning grids provided by the <code>train()</code> function, let's customize your models a bit more.</p>
<p>You can provide any number of values for <code>mtry</code>, from 2 up to the number of columns in the dataset.  In practice, there are diminishing returns for much larger values of <code>mtry</code>, so you will use a custom tuning grid that explores 2 simple models (<code>mtry = 2</code> and <code>mtry = 3</code>) as well as one more complicated model (<code>mtry = 7</code>).</p>
</div>

<li>Define a custom tuning grid.
<li>Set the number of variables to possibly split at each node, <code>.mtry</code>, to a vector of <code>2</code>, <code>3</code>, and <code>7</code>.</li>


<li>Set the rule to split on, <code>.splitrule</code>,  to <code>"variance"</code>.</li>


<li>Set the minimum node size, <code>.min.node.size</code>, to <code>5</code>.</li>


</li>



<li>Train another random forest model, <code>model</code>, using the <code>wine</code> dataset on the <code>quality</code> variable with all other variables as explanatory variables.
<li>Use <code>method = "ranger"</code>.</li>
<li>Use the custom <code>tuneGrid</code>.</li>
<li>Use 5 CV folds.</li>
</li>


<li>Print <code>model</code> to the console.</li>


<li>Plot the model after fitting it using <code>plot()</code>.</li>
```{r}
# Define the tuning grid: tuneGrid
tuneGrid <- data.frame(
  .mtry = c(2, 3, 7),
  .splitrule = "variance",
  .min.node.size = 5
)

# Fit random forest: model
model <- train(
  quality ~ .,
  tuneGrid = tuneGrid,
  data = wine, 
  method = "ranger",
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model

# Plot model
plot(model)
```

<p class="">Great work! Model tuning plots can be very useful for understanding caret models.
</p>


#### Introducing glmnet {.unnumbered}



##### Advantage of glmnet {.unnumbered}

<p>What's the advantage of <code>glmnet</code> over regular <code>glm</code> models?</p>


- [ ] <code>glmnet</code> models automatically find interaction variables.
- [ ] <code>glmnet</code> models don't provide p-values or confidence intervals on predictions.
- [x] <code>glmnet</code> models place constraints on your coefficients, which helps prevent overfitting.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Yes! glmnet models give you an easy way to optimize for simpler models.</p>

##### Make a custom trainControl {.unnumbered}


<div class>
<p>The wine quality dataset was a regression problem, but now you are looking at a classification problem. This is a simulated dataset based on the "don't overfit" competition on Kaggle a number of years ago.</p>
<p>Classification problems are a little more complicated than regression problems because you have to provide a custom <code>summaryFunction</code> to the <code>train()</code> function to use the <code>AUC</code> metric to rank your models. Start by making a custom <code>trainControl</code>, as you did in the previous chapter. Be sure to set <code>classProbs = TRUE</code>, otherwise the <code>twoClassSummary</code> for <code>summaryFunction</code> will break.</p>
</div>
<div class="exercise--instructions__content">
<p>Make a custom <code>trainControl</code> called <code>myControl</code> for classification using the <code>trainControl</code> function.</p>

<li>Use 10 CV folds.</li>


<li>Use <code>twoClassSummary</code> for the <code>summaryFunction</code>.</li>


<li>Be sure to set <code>classProbs = TRUE</code>.</li>



```{r}
# Create custom trainControl: myControl
myControl <- trainControl(
  method = "cv", 
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE
)
```

</div>

<p class="">Great work! Creating a custome trainControl gives you much finer control over how caret searches for models.
</p>

##### Fit glmnet with custom trainControl {.unnumbered}


<div class>
<p>Now that you have a custom <code>trainControl</code> object, fit a <code>glmnet</code> model to the "don't overfit" dataset. Recall from the video that <code>glmnet</code> is an extension of the generalized linear regression model (or <code>glm</code>) that places constraints on the magnitude of the coefficients to prevent overfitting. This is more commonly known as "penalized" regression modeling and is a very useful technique on datasets with many predictors and few values.</p>
<p><code>glmnet</code> is capable of fitting two different kinds of penalized models, controlled by the <code>alpha</code> parameter:</p>

<li>Ridge regression (or <code>alpha = 0</code>)</li>


<li>Lasso regression (or <code>alpha = 1</code>)</li>



<p>You'll now fit a <code>glmnet</code> model to the "don't overfit" dataset using the defaults provided by the <code>caret</code> package.</p>
</div>

<li>Train a <code>glmnet</code> model called <code>model</code> on the <code>overfit</code> data. Use the custom <code>trainControl</code> from the previous exercise (<code>myControl</code>). The variable <code>y</code> is the response variable and all other variables are explanatory variables.</li>


<li>Print the model to the console.</li>


<li>Use the <code>max()</code> function to find the maximum of the ROC statistic contained somewhere in <code>model[["results"]]</code>.</li>
```{r}
# edited/added
library(glmnet)
overfit=read.csv("https://assets.datacamp.com/production/repositories/223/datasets/0bd5f7c30d9aec3e1f1fa677a19bee3af407453a/overfit.csv")

# Fit glmnet model: model
model <- train(
  y ~ ., 
  overfit,
  method = "glmnet",
  trControl = myControl
)

# Print model to console
model

# Print maximum ROC statistic
max(model[["results"]][["ROC"]])
```

<p class="">Awesome job! This glmnet will use AUC rather than accuracy to select the final model parameters.
</p>


#### glmnet with custom tuning grid {.unnumbered}



##### Why a custom tuning grid? {.unnumbered}

<p>Why use a custom tuning grid for a <code>glmnet</code> model?</p>


- [ ] There's no reason to use a custom grid; the default is always the best.
- [x] The default tuning grid is very small and there are many more potential <code>glmnet</code> models you want to explore.
- [ ] <code>glmnet</code> models are really slow, so you should never try more than a few tuning parameters.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Good job! With a custom grid you can deeply explore machine learning models in caret.</p>

##### glmnet with custom trainControl and tuning {.unnumbered}


<div class>
<p>As you saw in the video, the <code>glmnet</code> model actually fits many models at once (one of the great things about the package). You can exploit this by passing a large number of <code>lambda</code> values, which control the amount of penalization in the model. <code>train()</code> is smart enough to only fit one model per <code>alpha</code> value and pass all of the <code>lambda</code> values at once for simultaneous fitting.</p>
<p>My favorite tuning grid for <code>glmnet</code> models is:</p>
<pre><code>expand.grid(
  alpha = 0:1,
  lambda = seq(0.0001, 1, length = 100)
)
</code></pre>
<p>This grid explores a large number of <code>lambda</code> values (100, in fact), from a very small one to a very large one. (You could increase the maximum <code>lambda</code> to 10, but in this exercise 1 is a good upper bound.)</p>
<p>If you want to explore fewer models, you can use a shorter lambda sequence.  For example, <code>lambda = seq(0.0001, 1, length = 10)</code> would fit 10 models per value of alpha.</p>
<p>You also look at the two forms of penalized models with this <code>tuneGrid</code>: ridge regression and lasso regression. <code>alpha = 0</code> is pure ridge regression, and <code>alpha = 1</code> is pure lasso regression. You can fit a mixture of the two models (i.e. an elastic net) using an <code>alpha</code> between 0 and 1. For example, <code>alpha = 0.05</code> would be 95% ridge regression and 5% lasso regression.</p>
<p>In this problem you'll just explore the 2 extremes  pure ridge and pure lasso regression  for the purpose of illustrating their differences.</p>
</div>

<li>Train a <code>glmnet</code> model on the <code>overfit</code> data such that <code>y</code> is the response variable and all other variables are explanatory variables. Make sure to use your custom <code>trainControl</code> from the previous exercise (<code>myControl</code>). Also, use a custom <code>tuneGrid</code> to explore <code>alpha = 0:1</code> and 20 values of <code>lambda</code> between 0.0001 and 1 per value of alpha.</li>


<li>Print <code>model</code> to the console.</li>


<li>Print the <code>max()</code> of the ROC statistic in <code>model[["results"]]</code>. You can access it using <code>model[["results"]][["ROC"]]</code>.</li>
```{r}
# Train glmnet with custom trainControl and tuning: model
model <- train(
  y ~ ., 
  overfit,
  tuneGrid = expand.grid(
    alpha = 0:1,
    lambda = seq(0.0001, 1, length = 20)
  ),
  method = "glmnet",
  trControl = myControl
)

# Print model to console
model

# Print maximum ROC statistic
max(model[["results"]][["ROC"]])
```

<p class="">Excellent work! I use this custom tuning grid for all my glmnet models  it's a great place to start!
</p>


##### Interpreting glmnet plots {.unnumbered}


<div class><p>Here's the tuning plot for the custom tuned <code>glmnet</code> model you created in the last exercise. For the <code>overfit</code> dataset, which value of <code>alpha</code> is better?</p></div>


- [ ] <code>alpha = 0</code> (ridge)
- [x] <code>alpha = 1</code> (lasso)


<p class="">Correct!  For this dataset, <code>alpha = 1</code> (or lasso) is better.
</p>

### Preprocessing {.unnumbered}

<p class="chapter__description">
    In this chapter, you will practice using <code>train()</code> to preprocess data before fitting models, improving your ability to making accurate predictions.
  </p>
  
#### Median imputation {.unnumbered}



##### Median imputation vs. omitting rows {.unnumbered}

<p>What's the value of median imputation?</p>


- [ ] It removes some variance from your data, making it easier to model.
- [x] It lets you model data with missing values.
- [ ] It's useless; you should just throw out rows of data with any missings.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct! Missing data can be a big headache unless you handle it.</p>

##### Apply median imputation {.unnumbered}


<div class>
<p>In this chapter, you'll be using a version of the Wisconsin Breast Cancer dataset. This dataset presents a classic binary classification problem: 50% of the samples are benign, 50% are malignant, and the challenge is to identify which are which.</p>
<p>This dataset is interesting because many of the predictors contain missing values and most rows of the dataset have at least one missing value. This presents a modeling challenge, because most machine learning algorithms cannot handle missing values out of the box. For example, your first instinct might be to fit a logistic regression model to this data, but prior to doing this you need a strategy for handling the <code>NA</code>s.</p>
<p>Fortunately, the <code>train()</code> function in <code>caret</code> contains an argument called <code>preProcess</code>, which allows you to specify that median imputation should be used to fill in the missing values. In previous chapters, you created models with the <code>train()</code> function using formulas such as <code>y ~ .</code>. An alternative way is to specify the <code>x</code> and <code>y</code> arguments to <code>train()</code>, where <code>x</code> is an object with samples in rows and features in columns and <code>y</code> is a numeric or factor vector containing the outcomes. Said differently, <code>x</code> is a matrix or data frame that contains the whole dataset you'd use for the <code>data</code> argument to the <code>lm()</code> call, for example, but excludes the response variable column; <code>y</code> is a vector that contains just the response variable column.</p>
<p>For this exercise, the argument <code>x</code> to <code>train()</code> is loaded in your workspace as <code>breast_cancer_x</code> and <code>y</code> as <code>breast_cancer_y</code>.</p>
</div>

<li>Use the <code>train()</code> function to fit a <code>glm</code> model called <code>median_model</code> to the breast cancer dataset. Use <code>preProcess = "medianImpute"</code> to handle the missing values.</li>


<li>Print <code>median_model</code> to the console.</li>
```{r}
# edited/added
load("archive/Machine-Learning-with-caret-in-R/datasets/BreastCancer.RData")

# Apply median imputation: median_model
median_model <- train(
  x = breast_cancer_x, 
  y = breast_cancer_y,
  method = "glm",
  trControl = myControl,
  preProcess = "medianImpute"
)

# Print median_model to console
median_model
```

<p class="">Fantastic job! Caret makes it very easy to include model preprocessing in your model validation.
</p>


#### KNN imputation {.unnumbered}



##### Comparing KNN imputation to median imputation {.unnumbered}

<p>Will KNN imputation always be better than median imputation?</p>


- [x] No, you should try both options and keep the one that gives more accurate models.
- [ ] Yes, KNN is a more complicated model than medians, so it's always better.
- [ ] No, medians are more statistically valid than KNN and should always be used.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct!  Always try everything and decide the best option empirically.</p>

##### Use KNN imputation {.unnumbered}


<div class>
<p>In the previous exercise, you used median imputation to fill in missing values in the breast cancer dataset, but that is not the only possible method for dealing with missing data.</p>
<p>An alternative to median imputation is k-nearest neighbors, or KNN, imputation. This is a more advanced form of imputation where missing values are replaced with values from other rows that are similar to the current row. While this is a lot more complicated to implement in practice than simple median imputation, it is very easy to explore in <code>caret</code> using the <code>preProcess</code> argument to <code>train()</code>. You can simply use <code>preProcess = "knnImpute"</code> to change the method of imputation used prior to model fitting.</p>
</div>
<div class="exercise--instructions__content">
<p><code>breast_cancer_x</code> and <code>breast_cancer_y</code> are loaded in your workspace.</p>

<li>Use the <code>train()</code> function to fit a <code>glm</code> model called <code>knn_model</code> to the breast cancer dataset.</li>


<li>Use KNN imputation to handle missing values.</li>
```{r}
# edited/added
library(RANN)

# Apply KNN imputation: knn_model
knn_model <- train(
  x = breast_cancer_x, 
  y = breast_cancer_y,
  method = "glm",
  trControl = myControl,
  preProcess = "knnImpute"
)

# Print knn_model to console
knn_model
```


</div>

<p class="">Good work! As you can see, you can easily try out different imputation methods.
</p>

##### Compare KNN and median imputation {.unnumbered}


<div class>
<p>All of the preprocessing steps in the <code>train()</code> function happen in the training set of each cross-validation fold, so the error metrics reported include the effects of the preprocessing.</p>
<p>This includes the imputation method used (e.g. <code>knnImpute</code> or <code>medianImpute</code>). This is useful because it allows you to compare different methods of imputation and choose the one that performs the best out-of-sample.</p>
<p><code>median_model</code> and <code>knn_model</code> are available in your workspace, as is <code>resamples</code>, which contains the resampled results of both models. Look at the results of the models by calling</p>
<pre><code>dotplot(resamples, metric = "ROC")
</code></pre>
<p>and choose the one that performs the best out-of-sample. Which method of imputation yields the highest out-of-sample ROC score for your <code>glm</code> model?</p>
</div>


- [ ] KNN imputation is much better than median imputation.
- [x] KNN imputation is slightly better than median imputation.
- [ ] Median imputation is much better than KNN imputation.
- [ ] Median imputation is slightly better than KNN imputation.


<p class="">Nice!
</p>

#### Multiple preprocessing methods {.unnumbered}



##### Order of operations {.unnumbered}

<p>Which comes first in <code>caret</code>'s <code>preProcess()</code> function: median imputation or centering and scaling of variables?</p>


- [x] Median imputation comes before centering and scaling.
- [ ] Centering and scaling come before median imputation.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct!  Centering and scaling require data with no missing values.</p>

##### Combining preprocessing methods {.unnumbered}


<div class>
<p>The <code>preProcess</code> argument to <code>train()</code> doesn't just limit you to imputing missing values. It also includes a wide variety of other <code>preProcess</code> techniques to make your life as a data scientist much easier. You can read a full list of them by typing <code>?preProcess</code> and reading the help page for this function.</p>
<p>One set of preprocessing functions that is particularly useful for fitting regression models is standardization: centering and scaling. You first <em>center</em> by subtracting the mean of each column from each value in that column, then you <em>scale</em> by dividing by the standard deviation.</p>
<p>Standardization transforms your data such that for each column, the mean is 0 and the standard deviation is 1. This makes it easier for regression models to find a good solution.</p>
</div>
<div class="exercise--instructions__content"><p><code>breast_cancer_x</code> and <code>breast_cancer_y</code> are loaded in your workspace. Fit a logistic regression model using median imputation called <code>model</code> to the breast cancer data, then print it to the console.</p></div>

<div class="exercise--instructions__content"><p>Update the model to include two more pre-processing steps: centering and scaling.</p></div>
```{r}
# Fit glm with median imputation
# Update model with standardization
model <- train(
  x = breast_cancer_x, 
  y = breast_cancer_y,
  method = "glm",
  trControl = myControl,
  preProcess = c("medianImpute", "center", "scale")
)

# Print updated model
model
```

<p class="">Great work! You can combine many different preprocessing methods with caret.
</p>

#### Handling low-information predictors {.unnumbered}



##### Why remove near zero variance predictors? {.unnumbered}

<p>What's the best reason to remove near zero variance predictors from your data before building a model?</p>


- [ ] Because they are guaranteed to have no effect on your model.
- [ ] Because their p-values in a linear regression will always be low.
- [x] To reduce model-fitting time without reducing model accuracy.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct!  Low variance variables are unlikely to have a large impact on our models.</p>

##### Remove near zero variance predictors {.unnumbered}


<div class>
<p>As you saw in the video, for the next set of exercises, you'll be using the blood-brain dataset. This is a biochemical dataset in which the task is to predict the following value for a set of biochemical compounds:</p>
<pre><code>log((concentration of compound in brain) /
      (concentration of compound in blood))
</code></pre>
<p>This gives a quantitative metric of the compound's ability to cross the blood-brain barrier, and is useful for understanding the biological properties of that barrier.</p>
<p>One interesting aspect of this dataset is that it contains many variables and many of these variables have extremely low variances. This means that there is very little information in these variables because they mostly consist of a single value (e.g. zero).</p>
<p>Fortunately, <code>caret</code> contains a utility function called <code>nearZeroVar()</code> for removing such variables to save time during modeling.</p>
<p><code>nearZeroVar()</code> takes in data <code>x</code>, then looks at the ratio of the most common value to the second most common value, <code>freqCut</code>, and the percentage of distinct values out of the number of total samples, <code>uniqueCut</code>. By default, <code>caret</code> uses <code>freqCut = 19</code> and <code>uniqueCut = 10</code>, which is fairly conservative. I like to be a little more aggressive and use <code>freqCut = 2</code> and <code>uniqueCut = 20</code> when calling <code>nearZeroVar()</code>.</p>
</div>
<div class="exercise--instructions__content">
<p><code>bloodbrain_x</code> and <code>bloodbrain_y</code> are loaded in your workspace.</p>

<li>Identify the near zero variance predictors by running <code>nearZeroVar()</code> on the blood-brain dataset. Store the result as an object called <code>remove_cols</code>. Use <code>freqCut = 2</code> and <code>uniqueCut = 20</code> in the call to <code>nearZeroVar()</code>.</li>


<li>Use <code>names()</code> to create a vector containing all column names of <code>bloodbrain_x</code>. Call this <code>all_cols</code>.</li>


<li>Make a new data frame called <code>bloodbrain_x_small</code> with the near-zero variance variables removed. Use <code>setdiff()</code> to isolate the column names that you wish to keep (i.e. that you don't want to remove.)</li>
```{r}
# edited/added
load("archive/Machine-Learning-with-caret-in-R/datasets/BloodBrain.Rdata")

# Identify near zero variance predictors: remove_cols
remove_cols <- nearZeroVar(bloodbrain_x, names = TRUE, 
                           freqCut = 2, uniqueCut = 20)

# Get all column names from bloodbrain_x: all_cols
all_cols <- names(bloodbrain_x)

# Remove from data: bloodbrain_x_small
bloodbrain_x_small <- bloodbrain_x[ , setdiff(all_cols, remove_cols)]
```


</div>

<p class="">Great work! Near zero variance variables can cause issues during cross-validation.
</p>

##### preProcess() and nearZeroVar() {.unnumbered}


<div class><p>Can you use the <code>preProcess</code> argument in <code>caret</code> to remove near-zero variance predictors?  Or do you have to do this by hand, prior to modeling, using the <code>nearZeroVar()</code> function?</p></div>


- [x] Yes!  Set the <code>preProcess</code> argument equal to <code>"nzv"</code>.
- [ ] No, unfortunately. You have to do this by hand.


<p class="">Yes!
</p>

##### Fit model on reduced blood-brain data {.unnumbered}


<div class>
<p>Now that you've reduced your dataset, you can fit a <code>glm</code> model to it using the <code>train()</code> function. This model will run faster than using the full dataset and will yield very similar predictive accuracy.</p>
<p>Furthermore, zero variance variables can cause problems with cross-validation (e.g. if one fold ends up with only a single unique value for that variable), so removing them prior to modeling means you are less likely to get errors during the fitting process.</p>
</div>
<div class="exercise--instructions__content">
<p><code>bloodbrain_x</code>, <code>bloodbrain_y</code>, <code>remove</code>, and <code>bloodbrain_x_small</code> are loaded in your workspace.</p>

<li>Fit a <code>glm</code> model using the <code>train()</code> function and the reduced blood-brain dataset you created in the previous exercise.</li>


<li>Print the result to the console.</li>
```{r}
# Fit model on reduced data: model
model <- train(
  x = bloodbrain_x_small, 
  y = bloodbrain_y, 
  method = "glm"
)

# Print model to console
model
```


</div>

<p class="">Excellent job!  As discussed previously, glm generates a lot of warnings about convergence, but they're never a big deal and you can use the out-of-sample accuracy to make sure your model makes good predictions.
</p>

#### Principle components analysis (PCA) {.unnumbered}



##### Using PCA as an alternative to nearZeroVar() {.unnumbered}


<div class>
<p>An alternative to removing low-variance predictors is to run PCA on your dataset. This is sometimes preferable because it does not throw out all of your data: many different low variance predictors may end up combined into one high variance PCA variable, which might have a positive impact on your model's accuracy.</p>
<p>This is an especially good trick for linear models: the <code>pca</code> option in the <code>preProcess</code> argument will center and scale your data, combine low variance variables, and ensure that all of your predictors are orthogonal. This creates an ideal dataset for linear regression modeling, and can often improve the accuracy of your models.</p>
</div>
<div class="exercise--instructions__content">
<p><code>bloodbrain_x</code> and <code>bloodbrain_y</code> are loaded in your workspace.</p>

<li>Fit a <code>glm</code> model to the full blood-brain dataset using the <code>"pca"</code> option to <code>preProcess</code>.</li>


<li>Print the model to the console and inspect the result.</li>
```{r}
# Fit glm model using PCA: model
model <- train(
  x = bloodbrain_x, 
  y = bloodbrain_y,
  method = "glm", 
  preProcess = "pca"
)

# Print model to console
model
```


</div>

<p class="">Great work! Note that the PCA model's accuracy is slightly higher than the <code>nearZeroVar()</code> model from the previous exercise. PCA is generally a better method for handling low-information predictors than throwing them out entirely.
</p>

### Selecting models {.unnumbered}

<p class="chapter__description">
    In the final chapter of this course, you'll learn how to use <code>resamples()</code> to compare multiple models and select (or ensemble) the best one(s).
  </p>
  
#### Reusing a trainControl {.unnumbered}



##### Why reuse a trainControl? {.unnumbered}

<p>Why reuse a <code>trainControl</code>?</p>


- [ ] So you can use the same <code>summaryFunction</code> and tuning parameters for multiple models.
- [ ] So you don't have to repeat code when fitting multiple models.
- [ ] So you can compare models on the exact same training and test data.
- [x] All of the above.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Nice one!</p>

##### Make custom train/test indices {.unnumbered}


<div class>
<p>As you saw in the video, for this chapter you will focus on a real-world dataset that brings together all of the concepts discussed in the previous chapters.</p>
<p>The churn dataset contains data on a variety of telecom customers and the modeling challenge is to predict which customers will cancel their service (or churn).</p>
<p>In this chapter, you will be exploring two different types of predictive models: <code>glmnet</code> and <code>rf</code>, so the first order of business is to create a reusable <code>trainControl</code> object you can use to reliably compare them.</p>
</div>
<div class="exercise--instructions__content">
<p><code>churn_x</code> and <code>churn_y</code> are loaded in your workspace.</p>

<li>Use <code>createFolds()</code> to create 5 CV folds on <code>churn_y</code>, your target variable for this exercise.</li>


<li>Pass them to <code>trainControl()</code> to create a reusable <code>trainControl</code> for comparing models.</li>
```{r}
# edited/added
load("archive/Machine-Learning-with-caret-in-R/datasets/Churn.RData")

# Create custom indices: myFolds
myFolds <- createFolds(churn_y, k = 5)

# Create reusable trainControl object: myControl
myControl <- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  verboseIter = TRUE,
  savePredictions = TRUE,
  index = myFolds
)
```


</div>

<p class="">Great work!  By saving the indexes in the train control, we can fit many models using the same CV folds.
</p>

#### Reintroducing glmnet {.unnumbered}



##### glmnet as a baseline model {.unnumbered}

<p>What makes <code>glmnet</code> a good baseline model?</p>


- [x] It's simple, fast, and easy to interpret.
- [ ] It always gives poor predictions, so your other models will look good by comparison.
- [ ] Linear models with penalties on their coefficients always give better results.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct! You can interpret the coefficients the same way as the coefficients from an <code>lm</code> or <code>glm</code> model.</p>

##### Fit the baseline model {.unnumbered}


<div class>
<p>Now that you have a reusable <code>trainControl</code> object called <code>myControl</code>, you can start fitting different predictive models to your churn dataset and evaluate their predictive accuracy.</p>
<p>You'll start with one of my favorite models, <code>glmnet</code>, which penalizes linear and logistic regression models on the size and number of coefficients to help prevent overfitting.</p>
</div>
<div class="exercise--instructions__content"><p>Fit a <code>glmnet</code> model to the churn dataset called <code>model_glmnet</code>. Make sure to use <code>myControl</code>, which you created in the first exercise and is available in your workspace, as the <code>trainControl</code> object.</p></div>
```{r}
# Fit glmnet model: model_glmnet
model_glmnet <- train(
  x = churn_x, 
  y = churn_y,
  metric = "ROC",
  method = "glmnet",
  trControl = myControl
)
```

<p class="">Great work! This model uses our custome CV folds and will be easily compared to other models.
</p>

#### Reintroducing random forest {.unnumbered}



##### Random forest drawback {.unnumbered}

<p>What's the drawback of using a random forest model for churn prediction?</p>


- [ ] Tree-based models are usually less accurate than linear models.
- [x] You no longer have model coefficients to help interpret the model.
- [ ] Nobody else uses random forests to predict churn.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Yup!  Random forests are a little bit harder to interpret than linear models, though it is still possible to understand them.</p>

##### Random forest with custom trainControl {.unnumbered}


<div class>
<p>Another one of my favorite models is the random forest, which combines an ensemble of non-linear decision trees into a highly flexible (and usually quite accurate) model.</p>
<p>Rather than using the classic <code>randomForest</code> package, you'll be using the <code>ranger</code> package, which is a re-implementation of <code>randomForest</code> that produces almost the exact same results, but is faster, more stable, and uses less memory. I highly recommend it as a starting point for random forest modeling in R.</p>
</div>
<div class="exercise--instructions__content">
<p><code>churn_x</code> and <code>churn_y</code> are loaded in your workspace.</p>

<li>Fit a random forest model to the churn dataset. Be sure to use <code>myControl</code> as the <code>trainControl</code> like you've done before and implement the <code>"ranger"</code> method.</li>
```{r}
# Fit random forest: model_rf
model_rf <- train(
  x = churn_x, 
  y = churn_y,
  metric = "ROC",
  method = "ranger",
  trControl = myControl
)
```


</div>

<p class="">Great work! This random forest uses the custom CV folds, so we can easily compare it to the baseline model.
</p>

#### Comparing models {.unnumbered}



##### Matching train/test indices {.unnumbered}

<p>What's the primary reason that train/test indices need to match when comparing two models?</p>


- [ ] You can save a lot of time when fitting your models because you don't have to remake the datasets.
- [ ] There's no real reason; it just makes your plots look better.
- [x] Because otherwise you wouldn't be doing a fair comparison of your models and your results could be due to chance.


<p class="dc-completion-pane__message dc-u-maxw-100pc">Correct!  Train/test indexes allow you to evaluate your models <em>out of sample</em> so you know that they work!</p>

##### Create a resamples object {.unnumbered}


<div class>
<p>Now that you have fit two models to the churn dataset, it's time to compare their out-of-sample predictions and choose which one is the best model for your dataset.</p>
<p>You can compare models in <code>caret</code> using the <code>resamples()</code> function, provided they have the same training data and use the same <code>trainControl</code> object with preset cross-validation folds.  <code>resamples()</code> takes as input a list of models and can be used to compare dozens of models at once (though in this case you are only comparing two models).</p>
</div>
<div class="exercise--instructions__content">
<p><code>model_glmnet</code> and <code>model_rf</code> are loaded in your workspace.</p>

<li>Create a <code>list()</code> containing the <code>glmnet</code> model as <code>item1</code> and the <code>ranger</code> model as <code>item2</code>.</li>


<li>Pass this list to the <code>resamples()</code> function and save the resulting object as <code>resamples</code>.</li>


<li>Summarize the results by calling <code>summary()</code> on <code>resamples</code>.</li>
```{r}
# Create model_list
model_list <- list(item1 = model_glmnet, item2 = model_rf)

# Pass model_list to resamples(): resamples
resamples <- resamples(model_list)

# Summarize the results
summary(resamples)
```


</div>

<p class="">Amazing! The resamples function gives us a bunch of options for comparing models, that we'll explore further in the next exercises.
</p>

#### More on resamples {.unnumbered}



##### Create a box-and-whisker plot {.unnumbered}


<div class>
<p><code>caret</code> provides a variety of methods to use for comparing models. All of these methods are based on the <code>resamples()</code> function. My favorite is the box-and-whisker plot, which allows you to compare the distribution of predictive accuracy (in this case AUC) for the two models.</p>
<p>In general, you want the model with the higher median AUC, as well as a smaller range between min and max AUC.</p>
<p>You can make this plot using the <code>bwplot()</code> function, which makes a box and whisker plot of the model's out of sample scores. Box and whisker plots show the median of each distribution as a line and the interquartile range of each distribution as a box around the median line. You can pass the <code>metric = "ROC"</code> argument to the <code>bwplot()</code> function to show a plot of the model's out-of-sample ROC scores and choose the model with the highest median ROC.</p>
<p>If you do not specify a metric to plot, <code>bwplot()</code> will automatically plot 3 of them.</p>
</div>
<div class="exercise--instructions__content"><p>Pass the <code>resamples</code> object to the <code>bwplot()</code> function to make a box-and-whisker plot. Look at the resulting plot and note which model has the higher median ROC statistic.  Be sure to specify which metric you want to plot.</p></div>
```{r}
# Create bwplot
bwplot(resamples, metric = "ROC")
```

<p class="">Great work! I'm a big fan of box and whisker plots for comparing models.
</p>

##### Create a scatterplot {.unnumbered}


<div class>
<p>Another useful plot for comparing models is the scatterplot, also known as the xy-plot. This plot shows you how similar the two models' performances are on different folds.</p>
<p>It's particularly useful for identifying if one model is consistently better than the other across all folds, or if there are situations when the inferior model produces better predictions on a particular subset of the data.</p>
</div>
<div class="exercise--instructions__content"><p>Pass the <code>resamples</code> object to the <code>xyplot()</code> function. Look at the resulting plot and note how similar the two models' predictions are (or are not) on the different folds.  Be sure to specify which metric you want to plot.</p></div>
```{r}
# Create xyplot
xyplot(resamples, metric = "ROC")
```

<p class="">Nice one! These scatterplots let you see if one model is always better than the other.
</p>

##### Ensembling models {.unnumbered}


<div class>
<p>That concludes the course!  As a teaser for a future course on making ensembles of <code>caret</code> models, I'll show you how to fit a stacked ensemble of models using the <code>caretEnsemble</code> package.</p>
<p><code>caretEnsemble</code> provides the <code>caretList()</code> function for creating multiple <code>caret</code> models at once on the same dataset, using the same resampling folds. You can also create your own lists of <code>caret</code> models.</p>
<p>In this exercise, I've made a <code>caretList</code> for you, containing the <code>glmnet</code> and <code>ranger</code> models you fit on the churn dataset. Use the <code>caretStack()</code> function to make a stack of <code>caret</code> models, with the two sub-models (<code>glmnet</code> and <code>ranger</code>) feeding into another (hopefully more accurate!) <code>caret</code> model.</p>
</div>

<li>Call the <code>caretStack()</code> function with two arguments, <code>model_list</code> and <code>method = "glm"</code>, to ensemble the two models using a logistic regression. Store the result as <code>stack</code>.</li>


<li>Summarize the resulting model with the <code>summary()</code> function.</li>
```{r}
# edited/added
library(caretEnsemble)
model_list <- c(item1 = model_glmnet, item2 = model_rf)

# Create ensemble model: stack
stack <- caretStack(model_list, method = "glm")

# Look at summary
summary(stack)
```

<p class="">Great work! The <code>caretEnsemble</code> package gives you an easy way to combine many caret models. Now for a brief farewell message from Max
</p>


#### Summary {.unnumbered}

##### Summary {.unnumbered}

We hope that you've enjoyed this course and found it helpful.

##### What you've learned {.unnumbered}

In summary, you've learned how to use R and the caret package to carry out the basic steps of model fitting and evaluation using out-of-sample error and cross-validation. You looked at how to tune model parameters for better results. And you applied data preprocessing techniques like median and knn imputation and PCA to avoid problems due to missing data or correlated predictors.

##### Goals of the caret package {.unnumbered}

A major goal of the caret package is to simplify many common steps in the predictive modeling process and to help you try different types of models and pre-processing techniques without being exposed to the specific syntax within each R package. This is just the beginning; each data set that you encounter is likely to have its own idiosyncrasies and might require different approaches. Fortunately, R has a wealth of predictive modeling algorithms that you can use to solve your problems.

##### Go build some models! {.unnumbered}

Thanks for spending time with us. Now go build some models!